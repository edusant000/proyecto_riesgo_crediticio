# --- Código extraído de: 05_Modelo_Avanzado.ipynb ---
# Modelado Avanzado Mejorado - Credit Scoring
import pandas as pd
import numpy as np
from pathlib import Path
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
from datetime import datetime

# Preprocesamiento y Pipelines
from sklearn.model_selection import (train_test_split, StratifiedKFold, 
                                     TimeSeriesSplit, cross_val_score)
from sklearn.preprocessing import StandardScaler, PowerTransformer, RobustScaler
from sklearn.pipeline import Pipeline
from sklearn.base import BaseEstimator, TransformerMixin
from imblearn.pipeline import Pipeline as ImbPipeline
from imblearn.combine import SMOTEENN
from imblearn.over_sampling import ADASYN
from sklearn.calibration import CalibratedClassifierCV
from sklearn.ensemble import IsolationForest

# Modelos Avanzados
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor, HistGradientBoostingRegressor
try:
    from xgboost import XGBClassifier, XGBRegressor
    from lightgbm import LGBMClassifier, LGBMRegressor
    from catboost import CatBoostClassifier, CatBoostRegressor
    ADVANCED_LIBS_AVAILABLE = True
except ImportError:
    print("Advertencia: XGBoost, LightGBM o CatBoost no disponibles. Usando modelos base.")
    ADVANCED_LIBS_AVAILABLE = False

try:
    import optuna
    OPTUNA_AVAILABLE = True
except ImportError:
    print("Advertencia: Optuna no disponible. Usando RandomizedSearchCV.")
    from sklearn.model_selection import RandomizedSearchCV
    OPTUNA_AVAILABLE = False

# Métricas y Interpretabilidad
from sklearn.metrics import (classification_report, roc_auc_score, f1_score, 
                             precision_recall_curve, auc, r2_score, 
                             mean_squared_error, precision_score, average_precision_score)
import shap
from scipy.stats import ks_2samp
from scipy.stats.mstats import winsorize

warnings.filterwarnings('ignore', category=FutureWarning)
sns.set_style("whitegrid")
plt.rcParams['figure.figsize'] = (12, 7)
RANDOM_STATE = 42

class TemporalFeatureEngineer(BaseEstimator, TransformerMixin):
    """
    Transformador que crea características temporales evitando data leakage.
    Solo usa información histórica previa al punto de predicción.
    """
    def __init__(self, lookback_months=3, target_month='sept'):
        self.lookback_months = lookback_months
        self.target_month = target_month
        self.month_order = ['april', 'may', 'june', 'july', 'aug', 'sept']
        
    def fit(self, X, y=None):
        return self
    
    def transform(self, X):
        X_transformed = X.copy()
        
        # Obtener índice del mes objetivo
        target_idx = self.month_order.index(self.target_month)
        historical_months = self.month_order[:target_idx][-self.lookback_months:]
        
        # Solo usar meses históricos para evitar data leakage
        pay_cols = [f'pay_amt_{month}' for month in historical_months]
        bill_cols = [f'bill_amt_{month}' for month in historical_months]
        pay_status_cols = [f'pay_{month}' for month in historical_months]
        
        # Características de tendencia temporal
        if len(pay_cols) >= 2:
            X_transformed['payment_trend'] = self._calculate_trend(X_transformed, pay_cols)
            X_transformed['bill_trend'] = self._calculate_trend(X_transformed, bill_cols)
            X_transformed['payment_volatility'] = X_transformed[pay_cols].std(axis=1)
            
        # Características de comportamiento de pago
        X_transformed['avg_payment_delay'] = X_transformed[pay_status_cols].mean(axis=1)
        X_transformed['months_with_delay'] = (X_transformed[pay_status_cols] > 0).sum(axis=1)
        X_transformed['consecutive_delays'] = self._calculate_consecutive_delays(X_transformed, pay_status_cols)
        
        # Ratios financieros
        epsilon = 1e-6
        recent_bill = f'bill_amt_{historical_months[-1]}'
        recent_pay = f'pay_amt_{historical_months[-1]}'
        
        X_transformed['utilization_recent'] = X_transformed[recent_bill] / (X_transformed['limit_bal'] + epsilon)
        X_transformed['payment_ratio_recent'] = X_transformed[recent_pay] / (X_transformed[recent_bill] + epsilon)
        X_transformed['debt_to_limit_ratio'] = X_transformed[bill_cols].mean(axis=1) / (X_transformed['limit_bal'] + epsilon)
        
        return X_transformed
    
    def _calculate_trend(self, df, cols):
        """Calcula la tendencia temporal usando regresión lineal simple."""
        trends = []
        for idx, row in df.iterrows():
            y_vals = row[cols].values.astype(float)
            if len(y_vals) > 1:
                x_vals = np.arange(len(y_vals))
                slope, _ = np.polyfit(x_vals, y_vals, 1)
                trends.append(slope)
            else:
                trends.append(0)
        return pd.Series(trends, index=df.index)
    
    def _calculate_consecutive_delays(self, df, pay_status_cols):
        """Calcula el número máximo de retrasos consecutivos."""
        consecutive_delays = []
        for idx, row in df.iterrows():
            delays = (row[pay_status_cols] > 0).astype(int).values
            max_consecutive = 0
            current_consecutive = 0
            
            for delay in delays:
                if delay == 1:
                    current_consecutive += 1
                    max_consecutive = max(max_consecutive, current_consecutive)
                else:
                    current_consecutive = 0
            
            consecutive_delays.append(max_consecutive)
        
        return pd.Series(consecutive_delays, index=df.index)

class AdaptiveOutlierHandler(BaseEstimator, TransformerMixin):
    """
    Maneja outliers de forma adaptativa usando Isolation Forest
    en lugar de winsorización fija.
    """
    def __init__(self, contamination=0.05, random_state=42):
        self.contamination = contamination
        self.random_state = random_state
        self.outlier_detectors = {}
        
    def fit(self, X, y=None):
        numeric_cols = X.select_dtypes(include=[np.number]).columns
        
        for col in numeric_cols:
            if X[col].std() > 0:  # Solo para columnas con variabilidad
                detector = IsolationForest(
                    contamination=self.contamination,
                    random_state=self.random_state
                )
                detector.fit(X[[col]])
                self.outlier_detectors[col] = detector
        
        return self
    
    def transform(self, X):
        X_transformed = X.copy()
        
        for col, detector in self.outlier_detectors.items():
            outlier_mask = detector.predict(X_transformed[[col]]) == -1
            if outlier_mask.sum() > 0:
                # Reemplazar outliers con percentiles
                p99 = X_transformed[col].quantile(0.99)
                p01 = X_transformed[col].quantile(0.01)
                
                X_transformed.loc[outlier_mask & (X_transformed[col] > p99), col] = p99
                X_transformed.loc[outlier_mask & (X_transformed[col] < p01), col] = p01
        
        return X_transformed

class BusinessMetricsCalculator:
    """
    Calcula métricas orientadas al negocio bancario.
    """
    def __init__(self, cost_fp=100, benefit_tp=500, cost_fn=1000):
        self.cost_fp = cost_fp  # Costo de falso positivo (rechazar buen cliente)
        self.benefit_tp = benefit_tp  # Beneficio de verdadero positivo (identificar riesgo)
        self.cost_fn = cost_fn  # Costo de falso negativo (aprobar mal cliente)
    
    def calculate_profit_curve(self, y_true, y_pred_proba):
        """Calcula la curva de profit para diferentes thresholds."""
        thresholds = np.linspace(0.01, 0.99, 100)
        profits = []
        
        for threshold in thresholds:
            y_pred = (y_pred_proba >= threshold).astype(int)
            profit = self._calculate_profit(y_true, y_pred)
            profits.append(profit)
        
        optimal_idx = np.argmax(profits)
        return thresholds, profits, thresholds[optimal_idx], profits[optimal_idx]
    
    def _calculate_profit(self, y_true, y_pred):
        """Calcula el profit total basado en la matriz de confusión."""
        tp = ((y_true == 1) & (y_pred == 1)).sum()
        fp = ((y_true == 0) & (y_pred == 1)).sum()
        fn = ((y_true == 1) & (y_pred == 0)).sum()
        
        profit = (tp * self.benefit_tp) - (fp * self.cost_fp) - (fn * self.cost_fn)
        return profit
    
    def precision_at_k(self, y_true, y_pred_proba, k_values=[0.1, 0.2, 0.3]):
        """Calcula precision at k para diferentes percentiles de riesgo."""
        metrics = {}
        
        for k in k_values:
            threshold = np.percentile(y_pred_proba, (1-k)*100)
            high_risk_mask = y_pred_proba >= threshold
            
            if high_risk_mask.sum() > 0:
                precision_k = y_true[high_risk_mask].mean()
                metrics[f'precision_at_{k}'] = precision_k
            else:
                metrics[f'precision_at_{k}'] = 0
        
        return metrics

def create_ensemble_model():
    """Crea un modelo ensemble diversificado."""
    models = []
    
    if ADVANCED_LIBS_AVAILABLE:
        # Modelo basado en árboles con regularización
        lgbm = LGBMClassifier(
            n_estimators=200,
            max_depth=6,
            learning_rate=0.1,
            subsample=0.8,
            colsample_bytree=0.8,
            reg_alpha=0.1,
            reg_lambda=0.1,
            random_state=RANDOM_STATE,
            verbose=-1
        )
        
        # Modelo con manejo automático de categóricas
        catboost = CatBoostClassifier(
            iterations=200,
            depth=6,
            learning_rate=0.1,
            l2_leaf_reg=3,
            random_state=RANDOM_STATE,
            verbose=False
        )
        
        models.extend([
            ('lgbm', lgbm),
            ('catboost', catboost)
        ])
    
    # Modelo lineal para diversidad
    log_reg = LogisticRegression(
        C=0.1,
        penalty='elasticnet',
        l1_ratio=0.5,
        solver='saga',
        max_iter=1000,
        random_state=RANDOM_STATE
    )
    
    # Random Forest con parámetros optimizados
    rf = RandomForestClassifier(
        n_estimators=200,
        max_depth=15,
        min_samples_split=10,
        min_samples_leaf=5,
        max_features='sqrt',
        class_weight='balanced_subsample',
        random_state=RANDOM_STATE,
        n_jobs=-1
    )
    
    models.extend([
        ('logistic', log_reg),
        ('random_forest', rf)
    ])
    
    return models

try:
    project_root = Path.cwd().parent
    processed_path = project_root / "data" / "processed"
    
    df_full = pd.read_csv(processed_path / "features_clasificacion.csv")
    df_reg = pd.read_csv(processed_path / "features_regresion.csv")
    
    print("Datasets cargados exitosamente.")
    print(f"Forma del dataset de clasificación: {df_full.shape}")
    print(f"Forma del dataset de regresión: {df_reg.shape}")

except FileNotFoundError as e:
    print(f"Error: No se encontró el archivo. Verifica la ruta: {e}")
    df_full = None
    df_reg = None

if df_full is not None:
    print("\n" + "="*70)
    print("MODELADO DE CLASIFICACIÓN AVANZADO")
    print("="*70)
    
    # Preparación de datos con validación temporal
    X = df_full.select_dtypes(include=np.number).drop(columns=['default_payment_next_month', 'ID'])
    y = df_full['default_payment_next_month']
    
    # Simular orden temporal basado en ID (asumiendo orden cronológico)
    df_full_sorted = df_full.sort_values('ID').reset_index(drop=True)
    split_point = int(len(df_full_sorted) * 0.8)
    
    # División temporal: 80% entrenamiento, 20% prueba
    X_train = df_full_sorted.iloc[:split_point].select_dtypes(include=np.number).drop(columns=['default_payment_next_month', 'ID'])
    X_test = df_full_sorted.iloc[split_point:].select_dtypes(include=np.number).drop(columns=['default_payment_next_month', 'ID'])
    y_train = df_full_sorted.iloc[:split_point]['default_payment_next_month']
    y_test = df_full_sorted.iloc[split_point:]['default_payment_next_month']
    
    print(f"División temporal - Entrenamiento: {X_train.shape[0]}, Prueba: {X_test.shape[0]}")
    
    # Pipeline mejorado con transformadores custom
    feature_engineer = TemporalFeatureEngineer(lookback_months=3, target_month='sept')
    outlier_handler = AdaptiveOutlierHandler(contamination=0.05)
    
    # Crear ensemble diversificado
    ensemble_models = create_ensemble_model()
    best_models = {}
    
    print("\n--- Entrenando Modelos del Ensemble ---")
    
    for name, base_model in ensemble_models:
        print(f"\nEntrenando {name}...")
        
        # Pipeline completo para cada modelo
        pipeline = ImbPipeline([
            ('feature_engineer', feature_engineer),
            ('outlier_handler', outlier_handler),
            ('sampler', ADASYN(random_state=RANDOM_STATE)),  # ADASYN en lugar de SMOTE
            ('scaler', RobustScaler()),  # RobustScaler para mejor manejo de outliers
            ('classifier', base_model)
        ])
        
        # Entrenamiento con validación cruzada temporal
        test_size_int = int(len(X_train) * 0.15) 
        tscv = TimeSeriesSplit(n_splits=5, test_size=test_size_int)

        cv_scores = cross_val_score(pipeline, X_train, y_train, cv=tscv, 
                                scoring='f1', n_jobs=-1)
        
        print(f"F1-Score CV: {cv_scores.mean():.4f} (+/- {cv_scores.std()*2:.4f})")
        
        # Entrenamiento final
        pipeline.fit(X_train, y_train)
        best_models[name] = pipeline
    
    # Selección del mejor modelo basado en métricas de negocio
    print("\n--- Evaluación y Selección del Mejor Modelo ---")
    business_calc = BusinessMetricsCalculator()
    model_scores = {}
    
    for name, model in best_models.items():
        y_pred_proba = model.predict_proba(X_test)[:, 1]
        
        # Métricas técnicas
        roc_auc = roc_auc_score(y_test, y_pred_proba)
        avg_precision = average_precision_score(y_test, y_pred_proba)
        
        # Métricas de negocio
        _, profits, optimal_threshold, max_profit = business_calc.calculate_profit_curve(y_test, y_pred_proba)
        precision_k = business_calc.precision_at_k(y_test, y_pred_proba)
        
        model_scores[name] = {
            'roc_auc': roc_auc,
            'avg_precision': avg_precision,
            'max_profit': max_profit,
            'optimal_threshold': optimal_threshold,
            **precision_k
        }
        
        print(f"\n{name.upper()}:")
        print(f"  ROC-AUC: {roc_auc:.4f}")
        print(f"  Avg Precision: {avg_precision:.4f}")
        print(f"  Max Profit: ${max_profit:,.0f}")
        print(f"  Optimal Threshold: {optimal_threshold:.3f}")
        print(f"  Precision@10%: {precision_k.get('precision_at_0.1', 0):.3f}")
    
    # Seleccionar mejor modelo basado en profit
    best_model_name = max(model_scores.keys(), 
                         key=lambda x: model_scores[x]['max_profit'])
    best_model = best_models[best_model_name]
    best_threshold = model_scores[best_model_name]['optimal_threshold']
    
    print(f"\n🏆 MEJOR MODELO: {best_model_name.upper()}")
    print(f"Threshold óptimo: {best_threshold:.3f}")
    
    # Calibración de probabilidades del mejor modelo
    print("\n--- Calibrando Probabilidades ---")
    
    # Extraer el clasificador del pipeline para calibración
    X_train_transformed = best_model[:-1].transform(X_train)
    base_classifier = best_model.named_steps['classifier']
    
    # Calibración con validación cruzada
    calibrated_clf = CalibratedClassifierCV(
        base_classifier, 
        method='isotonic', 
        cv=3
    )
    calibrated_clf.fit(X_train_transformed, y_train)
    
    # Predicciones finales calibradas
    X_test_transformed = best_model[:-1].transform(X_test)
    y_pred_proba_calibrated = calibrated_clf.predict_proba(X_test_transformed)[:, 1]
    y_pred_final = (y_pred_proba_calibrated >= best_threshold).astype(int)
    
    # Evaluación final
    print("\n--- EVALUACIÓN FINAL ---")
    print(classification_report(y_test, y_pred_final, 
                              target_names=['No Default', 'Default']))
    
    final_roc_auc = roc_auc_score(y_test, y_pred_proba_calibrated)
    print(f"\nROC-AUC Final (Calibrado): {final_roc_auc:.4f}")
    
    # Visualizaciones
    fig, axes = plt.subplots(2, 2, figsize=(15, 12))
    
    # 1. Curva Precision-Recall
    precision, recall, _ = precision_recall_curve(y_test, y_pred_proba_calibrated)
    pr_auc = auc(recall, precision)
    
    axes[0,0].plot(recall, precision, label=f'PR AUC = {pr_auc:.3f}')
    axes[0,0].set_xlabel('Recall')
    axes[0,0].set_ylabel('Precision')
    axes[0,0].set_title('Curva Precision-Recall')
    axes[0,0].legend()
    axes[0,0].grid(True)
    
    # 2. Curva de Profit
    thresholds, profits, opt_thresh, max_profit = business_calc.calculate_profit_curve(
        y_test, y_pred_proba_calibrated)
    
    axes[0,1].plot(thresholds, profits)
    axes[0,1].axvline(opt_thresh, color='red', linestyle='--', 
                     label=f'Óptimo: {opt_thresh:.3f}')
    axes[0,1].set_xlabel('Threshold')
    axes[0,1].set_ylabel('Profit ($)')
    axes[0,1].set_title('Curva de Profit')
    axes[0,1].legend()
    axes[0,1].grid(True)
    
    # 3. Distribución de Probabilidades
    axes[1,0].hist(y_pred_proba_calibrated[y_test == 0], alpha=0.7, 
                  label='No Default', bins=30)
    axes[1,0].hist(y_pred_proba_calibrated[y_test == 1], alpha=0.7, 
                  label='Default', bins=30)
    axes[1,0].axvline(best_threshold, color='red', linestyle='--', 
                     label=f'Threshold: {best_threshold:.3f}')
    axes[1,0].set_xlabel('Probabilidad Predicha')
    axes[1,0].set_ylabel('Frecuencia')
    axes[1,0].set_title('Distribución de Probabilidades')
    axes[1,0].legend()
    
    # 4. Matriz de Confusión Normalizada
    from sklearn.metrics import confusion_matrix
    cm = confusion_matrix(y_test, y_pred_final, normalize='true')
    sns.heatmap(cm, annot=True, fmt='.3f', cmap='Blues', ax=axes[1,1])
    axes[1,1].set_title('Matriz de Confusión (Normalizada)')
    axes[1,1].set_ylabel('Valor Real')
    axes[1,1].set_xlabel('Predicción')
    
    plt.tight_layout()
    plt.show()
    
    # Análisis de Interpretabilidad con SHAP
    print("\n--- Análisis de Interpretabilidad ---")

    # Usar una muestra un poco más grande para el fondo del explainer si es posible
    X_train_sample = X_train_transformed[:500]
    X_test_for_shap = X_test_transformed[:200]  # Muestra para eficiencia en predicción

    try:
        # 1. Crear el Explainer de SHAP
        if 'tree' in str(type(base_classifier)).lower():
            explainer = shap.TreeExplainer(base_classifier)
        else:
            # Usamos un background dataset para aproximar las expectativas
            explainer = shap.Explainer(base_classifier.predict_proba, X_train_sample)
        
        # 2. Calcular los valores SHAP
        # Para modelos de probabilidad, accedemos a la salida de la clase positiva (Default)
        shap_values = explainer(X_test_for_shap)
        
        # Asegurarnos de que estamos usando los valores de la clase 'Default'
        # La nueva API de SHAP a menudo devuelve un objeto explainer con .values y .base_values
        if isinstance(shap_values, list): # Para la API antigua
            shap_values_positive = shap_values[1]
        else: # Para la nueva API
            shap_values_positive = shap_values.values[:,:,1]

        # Obtener nombres de características (con el fallback que ya tenías)
        try:
            feature_names = best_model[:-1].get_feature_names_out()
        except Exception:
            print("Advertencia: No se pudieron obtener los nombres de las características del pipeline.")
            print("Se usarán nombres genéricos.")
            feature_names = [f"feature_{i}" for i in range(X_test_for_shap.shape[1])]

        # Convertir los datos de prueba a un DataFrame con los nombres correctos
        X_test_shap_df = pd.DataFrame(X_test_for_shap, columns=feature_names)

        # 3. Gráfico de Importancia (Corregido)
        # Usamos el summary plot por defecto (tipo 'dot'), que es más robusto e informativo.
        print("\nImportancia de Características (SHAP Summary Plot):")
        shap.summary_plot(shap_values_positive, X_test_shap_df, max_display=15)

        # 4. Gráfico de Dependencia/Interacción (Corregido y Añadido)
        # Este gráfico muestra cómo una característica impacta la predicción
        # y cómo interactúa con otra característica (elegida automáticamente por SHAP).
        print("\nGráfico de Dependencia (ej. para la característica más importante):")
        
        # Obtener la característica más importante para mostrar un ejemplo
        most_important_feature = X_test_shap_df.columns[np.argsort(np.abs(shap_values_positive).mean(0))[-1]]
        
        fig, ax = plt.subplots() # Crear figura para tener más control
        shap.dependence_plot(most_important_feature, shap_values_positive, X_test_shap_df, ax=ax)
        plt.tight_layout()  # <-- CLAVE: Ajusta el gráfico para que no se corte
        plt.show()          # <-- CLAVE: Muestra el gráfico ajustado

    except Exception as e:
        print(f"Error en análisis SHAP: {e}")
        print("Continuando sin análisis de interpretabilidad...")

if df_reg is not None:
    print("\n" + "="*70)
    print("MODELADO DE REGRESIÓN AVANZADO (HURDLE MODEL)")
    print("="*70)
    
    # Preparación de datos
    X_reg = df_reg.select_dtypes(include=np.number).drop(columns=['pay_amt_june', 'ID'])
    y_reg = df_reg['pay_amt_june']
    
    # Crear variable binaria para el hurdle (¿paga algo?)
    y_hurdle_binary = (y_reg > 0).astype(int)
    
    # División temporal
    df_reg_sorted = df_reg.sort_values('ID').reset_index(drop=True)
    split_point_reg = int(len(df_reg_sorted) * 0.8)
    
    X_train_reg = df_reg_sorted.iloc[:split_point_reg].select_dtypes(include=np.number).drop(columns=['pay_amt_june', 'ID'])
    X_test_reg = df_reg_sorted.iloc[split_point_reg:].select_dtypes(include=np.number).drop(columns=['pay_amt_june', 'ID'])
    y_train_reg = df_reg_sorted.iloc[:split_point_reg]['pay_amt_june']
    y_test_reg = df_reg_sorted.iloc[split_point_reg:]['pay_amt_june']
    y_train_binary = (y_train_reg > 0).astype(int)
    y_test_binary = (y_test_reg > 0).astype(int)
    
    # Datos solo para clientes que pagan (monto > 0)
    positive_mask_train = y_train_reg > 0
    positive_mask_test = y_test_reg > 0
    
    X_train_positive = X_train_reg[positive_mask_train]
    X_test_positive = X_test_reg[positive_mask_test]
    y_train_positive = y_train_reg[positive_mask_train]
    y_test_positive = y_test_reg[positive_mask_test]
    
    print(f"Clientes que pagan en entrenamiento: {positive_mask_train.sum()}")
    print(f"Clientes que pagan en prueba: {positive_mask_test.sum()}")
    
    # ETAPA 1: Modelo de Clasificación (Hurdle)
    print("\n--- ETAPA 1: Modelo Hurdle (¿Pagará algo?) ---")
    
    hurdle_pipeline = ImbPipeline([
        ('outlier_handler', AdaptiveOutlierHandler(contamination=0.03)),
        ('sampler', ADASYN(random_state=RANDOM_STATE)),
        ('scaler', RobustScaler()),
        ('classifier', LGBMClassifier(
            n_estimators=150,
            max_depth=6,
            learning_rate=0.1,
            random_state=RANDOM_STATE,
            verbose=-1
        ) if ADVANCED_LIBS_AVAILABLE else RandomForestClassifier(
            n_estimators=150, 
            max_depth=10, 
            random_state=RANDOM_STATE, 
            n_jobs=-1
        ))
    ])
    
    # Entrenamiento del modelo hurdle
    hurdle_pipeline.fit(X_train_reg, y_train_binary)
    
    # Evaluación del hurdle
    y_pred_hurdle = hurdle_pipeline.predict(X_test_reg)
    y_pred_hurdle_proba = hurdle_pipeline.predict_proba(X_test_reg)[:, 1]
    
    hurdle_auc = roc_auc_score(y_test_binary, y_pred_hurdle_proba)
    print(f"ROC-AUC Modelo Hurdle: {hurdle_auc:.4f}")
    print(f"Precisión Hurdle: {(y_pred_hurdle == y_test_binary).mean():.4f}")
    
    # ETAPA 2: Modelo de Regresión (¿Cuánto pagará?)
    print("\n--- ETAPA 2: Modelo de Monto ---")
    
    # Pipeline para regresión con manejo mejorado de outliers
    regression_pipeline = Pipeline([
        ('outlier_handler', AdaptiveOutlierHandler(contamination=0.05)),
        ('scaler', RobustScaler()),
        ('transformer', PowerTransformer(method='yeo-johnson')),
        ('regressor', HistGradientBoostingRegressor(
            max_iter=200,
            max_depth=8,
            learning_rate=0.1,
            l2_regularization=0.1,
            random_state=RANDOM_STATE
        ))
    ])
    
    # Entrenamiento con validación cruzada temporal
    test_size_reg_int = int(len(X_train_positive) * 0.2)
    tscv_reg = TimeSeriesSplit(n_splits=4, test_size=test_size_reg_int)

    rmse_scores = cross_val_score(
        regression_pipeline, 
        X_train_positive, 
        y_train_positive, 
        cv=tscv_reg, 
        scoring='neg_root_mean_squared_error',
        n_jobs=-1
    )
    
    print(f"RMSE CV: {-rmse_scores.mean():.2f} (+/- {rmse_scores.std()*2:.2f})")
    
    # Entrenamiento final
    regression_pipeline.fit(X_train_positive, y_train_positive)
    
    # COMBINACIÓN FINAL DEL MODELO HURDLE
    print("\n--- EVALUACIÓN FINAL DEL MODELO HURDLE COMBINADO ---")
    
    # Predicciones de la etapa 1 (¿pagará?)
    will_pay_proba = hurdle_pipeline.predict_proba(X_test_reg)[:, 1]
    will_pay_pred = will_pay_proba > 0.5
    
    # Predicciones de la etapa 2 (¿cuánto?)
    amount_pred = regression_pipeline.predict(X_test_reg)
    
    # Combinación: monto predicho * probabilidad de pago
    final_predictions = amount_pred * will_pay_proba
    
    # Métricas de evaluación
    r2_hurdle = r2_score(y_test_reg, final_predictions)
    rmse_hurdle = np.sqrt(mean_squared_error(y_test_reg, final_predictions))
    mae_hurdle = np.mean(np.abs(y_test_reg - final_predictions))
    
    # Métricas adicionales para evaluación de negocio
    def mean_absolute_percentage_error(y_true, y_pred):
        return np.mean(np.abs((y_true - y_pred) / np.maximum(y_true, 1))) * 100
    
    mape_hurdle = mean_absolute_percentage_error(y_test_reg, final_predictions)
    
    # Evaluar solo en clientes que realmente pagan
    if positive_mask_test.sum() > 0:
        r2_positive_only = r2_score(y_test_positive, final_predictions[positive_mask_test])
        rmse_positive_only = np.sqrt(mean_squared_error(y_test_positive, final_predictions[positive_mask_test]))
    else:
        r2_positive_only = rmse_positive_only = 0
    
    print(f"R² Score (Todos): {r2_hurdle:.4f}")
    print(f"RMSE (Todos): {rmse_hurdle:.2f}")
    print(f"MAE (Todos): {mae_hurdle:.2f}")
    print(f"MAPE (Todos): {mape_hurdle:.2f}%")
    print(f"R² Score (Solo pagadores): {r2_positive_only:.4f}")
    print(f"RMSE (Solo pagadores): {rmse_positive_only:.2f}")
    
    # Visualizaciones del modelo de regresión
    fig, axes = plt.subplots(2, 2, figsize=(15, 12))
    
    # 1. Predicciones vs Valores Reales (Todos)
    axes[0,0].scatter(y_test_reg, final_predictions, alpha=0.6, s=20)
    axes[0,0].plot([y_test_reg.min(), y_test_reg.max()], 
                   [y_test_reg.min(), y_test_reg.max()], '--r', linewidth=2)
    axes[0,0].set_xlabel('Valores Reales')
    axes[0,0].set_ylabel('Predicciones')
    axes[0,0].set_title(f'Predicciones vs Reales (Todos)\nR² = {r2_hurdle:.3f}')
    axes[0,0].grid(True, alpha=0.3)
    
    # 2. Predicciones vs Valores Reales (Solo pagadores)
    if positive_mask_test.sum() > 0:
        axes[0,1].scatter(y_test_positive, final_predictions[positive_mask_test], 
                         alpha=0.6, s=20, color='green')
        axes[0,1].plot([y_test_positive.min(), y_test_positive.max()], 
                       [y_test_positive.min(), y_test_positive.max()], '--r', linewidth=2)
        axes[0,1].set_xlabel('Valores Reales')
        axes[0,1].set_ylabel('Predicciones')
        axes[0,1].set_title(f'Predicciones vs Reales (Pagadores)\nR² = {r2_positive_only:.3f}')
        axes[0,1].grid(True, alpha=0.3)
    
    # 3. Distribución de Residuos
    residuals = y_test_reg - final_predictions
    axes[1,0].hist(residuals, bins=50, alpha=0.7, edgecolor='black')
    axes[1,0].axvline(residuals.mean(), color='red', linestyle='--', 
                     label=f'Media: {residuals.mean():.2f}')
    axes[1,0].set_xlabel('Residuos')
    axes[1,0].set_ylabel('Frecuencia')
    axes[1,0].set_title('Distribución de Residuos')
    axes[1,0].legend()
    axes[1,0].grid(True, alpha=0.3)
    
    # 4. Q-Q Plot de Residuos
    from scipy import stats
    stats.probplot(residuals, dist="norm", plot=axes[1,1])
    axes[1,1].set_title('Q-Q Plot de Residuos')
    axes[1,1].grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.show()
    
    # Análisis de Importancia de Características
    print("\n--- Análisis de Importancia de Características ---")
    
    # Importancia del modelo hurdle (clasificación)
    if hasattr(hurdle_pipeline.named_steps['classifier'], 'feature_importances_'):
        # Obtener datos transformados para obtener nombres correctos
        X_transformed_sample = hurdle_pipeline[:-1].transform(X_train_reg[:100])
        hurdle_importances = hurdle_pipeline.named_steps['classifier'].feature_importances_
        
        # Crear DataFrame de importancias del hurdle
        try:
            hurdle_feature_names = hurdle_pipeline[:-1].get_feature_names_out()
        except Exception:
            print("Advertencia: No se pudieron obtener los nombres de las características del pipeline del hurdle.")
            hurdle_feature_names = [f'feature_{i}' for i in range(len(hurdle_importances))]

        hurdle_importance_df = pd.DataFrame({
            'feature': hurdle_feature_names,
            'importance': hurdle_importances,
            'model': 'Hurdle (¿Pagará?)'
        }).sort_values('importance', ascending=False)

    print("Top 10 características más importantes para el modelo Hurdle:")
    print(hurdle_importance_df.head(10))
    
    # Importancia del modelo de regresión
    if hasattr(regression_pipeline.named_steps['regressor'], 'feature_importances_'):
        reg_importances = regression_pipeline.named_steps['regressor'].feature_importances_
        
        try:
            reg_feature_names = regression_pipeline[:-1].get_feature_names_out()
        except Exception:
            print("Advertencia: No se pudieron obtener los nombres de las características del pipeline de regresión.")
            reg_feature_names = [f'feature_{i}' for i in range(len(reg_importances))]
            
        reg_importance_df = pd.DataFrame({
            'feature': reg_feature_names,
            'importance': reg_importances,
            'model': 'Regresión (¿Cuánto?)'
        }).sort_values('importance', ascending=False)
        
        print("\nTop 10 características más importantes para el modelo de Regresión:")
        print(reg_importance_df.head(10))
        
        # Visualización comparativa de importancias
        if 'hurdle_importance_df' in locals():
            plt.figure(figsize=(15, 8))
            
            # Subplot para modelo hurdle
            plt.subplot(1, 2, 1)
            top_hurdle = hurdle_importance_df.head(15)
            plt.barh(range(len(top_hurdle)), top_hurdle['importance'])
            plt.yticks(range(len(top_hurdle)), top_hurdle['feature'])
            plt.xlabel('Importancia')
            plt.title('Importancia - Modelo Hurdle')
            plt.gca().invert_yaxis()
            
            # Subplot para modelo de regresión
            plt.subplot(1, 2, 2)
            top_reg = reg_importance_df.head(15)
            plt.barh(range(len(top_reg)), top_reg['importance'])
            plt.yticks(range(len(top_reg)), top_reg['feature'])
            plt.xlabel('Importancia')
            plt.title('Importancia - Modelo Regresión')
            plt.gca().invert_yaxis()
            
            plt.tight_layout()
            plt.show()

def analyze_population_stability(X_train, X_test, feature_cols=None, n_bins=10):
    """
    Calcula el Population Stability Index (PSI) para detectar drift.
    """
    if feature_cols is None:
        feature_cols = X_train.select_dtypes(include=[np.number]).columns
    
    psi_results = {}
    
    for col in feature_cols:
        if col in X_train.columns and col in X_test.columns:
            # Crear bins basados en los datos de entrenamiento
            _, bin_edges = np.histogram(X_train[col], bins=n_bins)
            
            # Calcular distribuciones
            train_dist, _ = np.histogram(X_train[col], bins=bin_edges, density=True)
            test_dist, _ = np.histogram(X_test[col], bins=bin_edges, density=True)
            
            # Normalizar para obtener proporciones
            train_prop = train_dist / train_dist.sum()
            test_prop = test_dist / test_dist.sum()
            
            # Evitar log(0) añadiendo pequeño epsilon
            epsilon = 1e-10
            train_prop = np.maximum(train_prop, epsilon)
            test_prop = np.maximum(test_prop, epsilon)
            
            # Calcular PSI
            psi = np.sum((test_prop - train_prop) * np.log(test_prop / train_prop))
            psi_results[col] = psi
    
    return psi_results

if df_full is not None and 'X_train' in locals():
    print("\n" + "="*70)
    print("ANÁLISIS DE ESTABILIDAD POBLACIONAL")
    print("="*70)
    
    # Seleccionar características originales para análisis de estabilidad
    original_features = ['limit_bal', 'age', 'bill_amt_sept', 'pay_amt_sept', 
                        'pay_sept', 'sex', 'education', 'marriage']
    available_features = [col for col in original_features if col in X_train.columns and col in X_test.columns]
    
    if available_features:
        psi_scores = analyze_population_stability(X_train, X_test, available_features)
        
        # Crear DataFrame de resultados PSI
        psi_df = pd.DataFrame([
            {'feature': k, 'psi': v, 'stability': 
             'Estable' if v < 0.1 else 'Moderado' if v < 0.2 else 'Inestable'} 
            for k, v in psi_scores.items()
        ]).sort_values('psi', ascending=False)
        
        print("Population Stability Index (PSI) por característica:")
        print("PSI < 0.1: Estable | 0.1-0.2: Moderado | >0.2: Inestable")
        print("-" * 60)
        for _, row in psi_df.iterrows():
            print(f"{row['feature']:20} | {row['psi']:6.3f} | {row['stability']}")
        
        # Visualización de estabilidad
        plt.figure(figsize=(12, 6))
        colors = ['green' if x < 0.1 else 'orange' if x < 0.2 else 'red' 
                 for x in psi_df['psi']]
        
        plt.barh(range(len(psi_df)), psi_df['psi'], color=colors)
        plt.yticks(range(len(psi_df)), psi_df['feature'])
        plt.xlabel('Population Stability Index (PSI)')
        plt.title('Análisis de Estabilidad de Variables')
        plt.axvline(x=0.1, color='orange', linestyle='--', alpha=0.7, label='Umbral Moderado')
        plt.axvline(x=0.2, color='red', linestyle='--', alpha=0.7, label='Umbral Inestable')
        plt.legend()
        plt.grid(True, alpha=0.3)
        plt.tight_layout()
        plt.show()
        
        # Recomendaciones basadas en PSI
        unstable_features = psi_df[psi_df['psi'] > 0.2]['feature'].tolist()
        if unstable_features:
            print(f"\n⚠️  ALERTA: Las siguientes variables muestran drift significativo:")
            for feature in unstable_features:
                print(f"   - {feature}")
            print("\nRecomendaciones:")
            print("1. Reentrenar el modelo con datos más recientes")
            print("2. Considerar recalibración de probabilidades")
            print("3. Implementar monitoreo continuo de estas variables")

print("\n" + "="*70)
print("RESUMEN EJECUTIVO")
print("="*70)

if 'best_model_name' in locals():
    print(f"\n🎯 MODELO DE CLASIFICACIÓN SELECCIONADO: {best_model_name.upper()}")
    print(f"   • ROC-AUC: {model_scores[best_model_name]['roc_auc']:.3f}")
    print(f"   • Precision@10%: {model_scores[best_model_name].get('precision_at_0.1', 0):.3f}")
    print(f"   • Profit Máximo: ${model_scores[best_model_name]['max_profit']:,.0f}")
    print(f"   • Threshold Óptimo: {best_threshold:.3f}")

if 'r2_hurdle' in locals():
    print(f"\n📈 MODELO DE REGRESIÓN HURDLE:")
    print(f"   • R² Score: {r2_hurdle:.3f}")
    print(f"   • RMSE: ${rmse_hurdle:,.0f}")
    print(f"   • MAPE: {mape_hurdle:.1f}%")
    print(f"   • ROC-AUC Hurdle: {hurdle_auc:.3f}")

print(f"\n📊 MEJORAS IMPLEMENTADAS:")
print("   ✅ Validación temporal para evitar data leakage")
print("   ✅ Ingeniería de características sin información futura")
print("   ✅ Manejo adaptativo de outliers con Isolation Forest")
print("   ✅ Ensemble diversificado con múltiples algoritmos")
print("   ✅ Métricas orientadas al negocio (profit optimization)")
print("   ✅ Análisis de estabilidad poblacional (PSI)")
print("   ✅ Calibración de probabilidades mejorada")

print(f"\n🚀 RECOMENDACIONES PARA PRODUCCIÓN:")
print("   1. Implementar monitoreo continuo de PSI")
print("   2. Recalibrar modelos mensualmente")
print("   3. Validar performance en cohortes temporales")
print("   4. Establecer alertas automáticas por drift")
print("   5. Documentar decisiones para auditoría regulatoria")
print("   6. Implementar A/B testing para nuevas versiones")

print(f"\n💡 PRÓXIMOS PASOS:")
print("   • Integración con pipeline de MLOps")
print("   • Desarrollo de API REST para scoring")
print("   • Dashboard de monitoreo en tiempo real")
print("   • Documentación para compliance regulatorio")

print("\n" + "="*70)
print("ANÁLISIS COMPLETADO ✅")
print("="*70)

# --- Código extraído de: 03_Modelado_Clasificacion.ipynb ---
import pandas as pd
import numpy as np
from pathlib import Path
import matplotlib.pyplot as plt
import seaborn as sns

# Se añade TimeSeriesSplit a las importaciones
from sklearn.model_selection import train_test_split, TimeSeriesSplit, RandomizedSearchCV
from sklearn.preprocessing import StandardScaler
from imblearn.over_sampling import SMOTE
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from sklearn.metrics import classification_report, roc_auc_score, accuracy_score, f1_score, precision_recall_curve, auc
from sklearn.base import clone # Importación clave para la solución

try:
    project_root = Path.cwd().parent
    processed_path = project_root / "data" / "processed"
    
    df_full = pd.read_csv(processed_path / "features_clasificacion.csv")
    df_reduced = pd.read_csv(processed_path / "features_reducido_clasificacion.csv")
    
    print("Datasets cargados exitosamente.")
    print(f"Forma del dataset completo: {df_full.shape}")
    print(f"Forma del dataset reducido: {df_reduced.shape}")

except FileNotFoundError as e:
    print(f"Error: No se encontró el archivo. Verifica la ruta: {e}")
    df_full = None
    df_reduced = None

def train_and_evaluate_model(X_train, X_test, y_train, y_test, model, model_name, dataset_name):
    """
    Entrena y evalúa un modelo de clasificación en datos ya divididos temporalmente.
    """
    feature_names_list = X_train.columns.tolist()

    smote = SMOTE(random_state=42)
    X_train_res, y_train_res = smote.fit_resample(X_train, y_train)

    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train_res)
    X_test_scaled = scaler.transform(X_test)

    model.fit(X_train_scaled, y_train_res)

    y_pred = model.predict(X_test_scaled)
    y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]

    metrics = {
        'Model': model_name,
        'Dataset': dataset_name,
        'Accuracy': accuracy_score(y_test, y_pred),
        'ROC-AUC': roc_auc_score(y_test, y_pred_proba),
        'F1-Score (Default)': f1_score(y_test, y_pred, pos_label=1),
        'artifacts': {
            'feature_names': feature_names_list,
            'y_test': y_test,
            'y_pred_proba': y_pred_proba,
            'model': model,
            
            # --- LÍNEAS AÑADIDAS DE VUELTA ---
            # Guardar los datos necesarios para la optimización de hiperparámetros.
            'X_train_scaled': X_train_scaled,
            'y_train_res': y_train_res,
            'X_test_scaled': X_test_scaled
            # --- FIN DE LAS LÍNEAS AÑADIDAS ---
        }
    }

    print(f"\n--- Resultados para {model_name} en {dataset_name} ---")
    print(classification_report(y_test, y_pred, target_names=['No Incumplimiento', 'Incumplimiento']))

    return metrics

if df_full is not None and df_reduced is not None:
    
    datasets_dfs = {
        "Completo": df_full,
        "Reducido": df_reduced
    }
    
    # Se renombra a "prototypes" para indicar que son plantillas a clonar
    model_prototypes = {
        "Regresión Logística": LogisticRegression(random_state=42, max_iter=1000),
        "Random Forest": RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1),
        "XGBoost": XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)
    }
    
    results = []
    
    for d_name, df_data in datasets_dfs.items():
        df_sorted = df_data.sort_values('ID').reset_index(drop=True)
        
        split_point = int(len(df_sorted) * 0.8)
        train_df = df_sorted.iloc[:split_point]
        test_df = df_sorted.iloc[split_point:]
        
        target_col = 'default_payment_next_month'
        features_to_use = df_data.select_dtypes(include=np.number).columns.drop([target_col, 'ID'], errors='ignore').tolist()
        
        X_train = train_df[features_to_use]
        y_train = train_df[target_col]
        X_test = test_df[features_to_use]
        y_test = test_df[target_col]
        
        # Iterar sobre cada prototipo de modelo
        for m_name, model_proto in model_prototypes.items():
            # --- SOLUCIÓN IMPLEMENTADA ---
            # Clonar el modelo para asegurar una instancia limpia en cada entrenamiento
            model_instance = clone(model_proto)
            
            metrics = train_and_evaluate_model(X_train, X_test, y_train, y_test, model_instance, m_name, d_name)
            results.append(metrics)
            
    results_df = pd.DataFrame(results)
    
    print("\n--- Tabla Comparativa de Rendimiento de Modelos ---")
    display(results_df.drop(columns=['artifacts']))

if 'results_df' in locals() and not results_df.empty:
    best_result = results_df.loc[results_df['F1-Score (Default)'].idxmax()]
    best_model_name = best_result['Model']
    best_dataset_name = best_result['Dataset']
    
    print(f"\n--- Análisis Profundo del Mejor Modelo: {best_model_name} en Dataset {best_dataset_name} ---")
    
    artifacts = best_result['artifacts']
    model_best = artifacts['model']
    feature_names = artifacts['feature_names']
    y_test_best = artifacts['y_test']
    y_pred_proba_best = artifacts['y_pred_proba']

    if hasattr(model_best, 'feature_importances_'):
        importances = model_best.feature_importances_
        
        # Ahora las longitudes coincidirán perfectamente
        feature_importance_df = pd.DataFrame({'feature': feature_names, 'importance': importances})
        feature_importance_df = feature_importance_df.sort_values(by='importance', ascending=False)
        
        plt.figure(figsize=(10, 12))
        sns.barplot(x='importance', y='feature', data=feature_importance_df.head(20), palette='mako')
        plt.title(f'Top 20 Características más Importantes ({best_model_name} - {best_dataset_name})')
        plt.xlabel('Importancia')
        plt.ylabel('Característica')
        plt.tight_layout()
        plt.show()
        
    precision, recall, _ = precision_recall_curve(y_test_best, y_pred_proba_best)
    pr_auc = auc(recall, precision)
    
    plt.figure(figsize=(8, 6))
    plt.plot(recall, precision, color='darkorange', lw=2, label=f'Curva PR (área = {pr_auc:.2f})')
    plt.xlabel('Recall')
    plt.ylabel('Precisión')
    plt.title('Curva de Precisión-Recall del Mejor Modelo')
    plt.legend(loc="lower left")
    plt.grid(True)
    plt.show()

if 'results_df' in locals() and not results_df.empty:
    print("\n--- Iniciando Optimización de Hiperparámetros para Random Forest ---")
    
    # Usar los artefactos del mejor modelo identificado
    artifacts_best = results_df.loc[results_df['F1-Score (Default)'].idxmax()]['artifacts']
    X_train_scaled_best = artifacts_best['X_train_scaled']
    y_train_res_best = artifacts_best['y_train_res']
    X_test_scaled_best = artifacts_best['X_test_scaled']
    y_test_best = artifacts_best['y_test']
    
    param_distributions = {
        'n_estimators': [100, 200, 300, 400],
        'max_depth': [10, 20, 30, None],
        'min_samples_split': [2, 5, 10],
        'min_samples_leaf': [1, 2, 4],
        'max_features': ['sqrt', 'log2']
    }
    
    rf_clf = RandomForestClassifier(random_state=42, n_jobs=-1)
    tscv = TimeSeriesSplit(n_splits=3)
    
    random_search = RandomizedSearchCV(
        estimator=rf_clf,
        param_distributions=param_distributions,
        n_iter=50,
        cv=tscv,
        scoring='f1',
        verbose=2,
        random_state=42,
        n_jobs=-1
    )
    
    random_search.fit(X_train_scaled_best, y_train_res_best)
    
    print("\n--- Mejores Hiperparámetros Encontrados ---")
    print(random_search.best_params_)
    
    best_rf_clf = random_search.best_estimator_
    y_pred_best_rf = best_rf_clf.predict(X_test_scaled_best)
    y_pred_proba_best_rf = best_rf_clf.predict_proba(X_test_scaled_best)[:, 1]
    
    print("\n--- Métricas de Evaluación del Modelo Random Forest OPTIMIZADO ---")
    print(f"Accuracy: {accuracy_score(y_test_best, y_pred_best_rf):.4f}")
    print(f"ROC-AUC Score: {roc_auc_score(y_test_best, y_pred_proba_best_rf):.4f}")
    print("\nReporte de Clasificación (Optimizado):")
    print(classification_report(y_test_best, y_pred_best_rf, target_names=['No Incumplimiento', 'Incumplimiento']))

# --- Código extraído de: 01_Analisis_Exploratorio.ipynb ---
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path

sns.set_style("whitegrid")
plt.rcParams['figure.figsize'] = (10, 6)

try:
    project_root = Path.cwd().parent
    data_file_path = project_root / "data" / "raw" / "default_of_credit_card_clients.xls"
    
    if data_file_path.exists():
        print(f"Archivo encontrado en: {data_file_path}")
        # Se lee el archivo Excel, omitiendo la primera fila que no contiene datos.
        df_original = pd.read_excel(data_file_path, skiprows=1)
    else:
        print(f"Error: No se encontró el archivo en la ruta esperada: {data_file_path}")
        df_original = None

except Exception as e:
    print(f"Ocurrió un error al cargar el archivo: {e}")
    df_original = None

if df_original is not None:
    print("\n--- Vista Previa de los Datos Originales ---")
    display(df_original.head())
    
    print("\n--- Información General del DataFrame ---")
    df_original.info()
else:
    print("\nNo se pudo cargar el DataFrame. El análisis no puede continuar.")

df = df_original.copy()

df.rename(columns={
    'LIMIT_BAL': 'limit_bal', 'SEX': 'sex', 'EDUCATION': 'education', 'MARRIAGE': 'marriage', 'AGE': 'age',
    'PAY_0': 'pay_sept', 'PAY_2': 'pay_aug', 'PAY_3': 'pay_july', 'PAY_4': 'pay_june', 'PAY_5': 'pay_may', 'PAY_6': 'pay_april',
    'BILL_AMT1': 'bill_amt_sept', 'BILL_AMT2': 'bill_amt_aug', 'BILL_AMT3': 'bill_amt_july', 'BILL_AMT4': 'bill_amt_june', 'BILL_AMT5': 'bill_amt_may', 'BILL_AMT6': 'bill_amt_april',
    'PAY_AMT1': 'pay_amt_sept', 'PAY_AMT2': 'pay_amt_aug', 'PAY_AMT3': 'pay_amt_july', 'PAY_AMT4': 'pay_amt_june', 'PAY_AMT5': 'pay_amt_may', 'PAY_AMT6': 'pay_amt_april',
    'default payment next month': 'default_payment_next_month'
}, inplace=True)

# 'education': Se agrupan valores no documentados (0, 5, 6) en la categoría 'Otros' (4).
df['education'] = df['education'].replace([0, 5, 6], 4)
# 'marriage': Se agrupa el valor no documentado (0) en 'Otros' (3).
df['marriage'] = df['marriage'].replace(0, 3)

print("Education:", sorted(df['education'].unique()))
print("Marriage:", sorted(df['marriage'].unique()))

display(df.describe())

default_counts = df['default_payment_next_month'].value_counts()
print(default_counts)

plt.figure(figsize=(6, 4))
sns.countplot(x='default_payment_next_month', data=df, palette='viridis')
plt.title('Distribución de Clientes por Incumplimiento de Pago')
plt.xlabel('Incumplimiento (0 = No, 1 = Sí)')
plt.ylabel('Cantidad de Clientes')
plt.xticks([0, 1], ['No Incumplimiento (78%)', 'Incumplimiento (22%)'])
plt.show()

df['sex_label'] = df['sex'].map({1: 'Hombre', 2: 'Mujer'})
df['education_label'] = df['education'].map({1: 'Posgrado', 2: 'Universidad', 3: 'Secundaria', 4: 'Otros'})
df['marriage_label'] = df['marriage'].map({1: 'Casado/a', 2: 'Soltero/a', 3: 'Otros'})

fig, axes = plt.subplots(1, 3, figsize=(18, 5))
sns.countplot(x='sex_label', data=df, ax=axes[0], palette='pastel')
axes[0].set_title('Distribución por Género')
sns.countplot(x='education_label', data=df, ax=axes[1], palette='pastel', order=df['education_label'].value_counts().index)
axes[1].set_title('Distribución por Nivel Educativo')
axes[1].tick_params(axis='x', rotation=45)
sns.countplot(x='marriage_label', data=df, ax=axes[2], palette='pastel')
axes[2].set_title('Distribución por Estado Civil')
plt.suptitle('Análisis de Variables Demográficas', fontsize=16)
plt.tight_layout(rect=[0, 0.03, 1, 0.95])
plt.show()

fig, axes = plt.subplots(1, 2, figsize=(15, 5))
sns.histplot(df['limit_bal'], kde=True, ax=axes[0], bins=30, color='skyblue')
axes[0].set_title('Distribución del Límite de Crédito')
axes[0].set_xlabel('Límite de Crédito (NT$)')
sns.histplot(df['age'], kde=True, ax=axes[1], bins=30, color='salmon')
axes[1].set_title('Distribución de la Edad')
axes[1].set_xlabel('Edad')
plt.suptitle('Análisis de Variables Numéricas', fontsize=16)
plt.show()

df['pay_sept_label'] = df['pay_sept'].map({
    -1: 'Pago puntual', 0: 'Pago mínimo', 1: 'Retraso 1 mes',
    2: 'Retraso 2 meses', 3: 'Retraso 3 meses', 4: 'Retraso 4 meses',
    5: 'Retraso 5 meses', 6: 'Retraso 6 meses', 7: 'Retraso 7 meses',
    8: 'Retraso >8 meses'
}).fillna('Otro')

plt.figure(figsize=(12, 7))
sns.countplot(y='pay_sept_label', data=df, order=df['pay_sept_label'].value_counts().index, palette='plasma')
plt.title('Estado de Pago en Septiembre de 2005')
plt.xlabel('Cantidad de Clientes')
plt.ylabel('Estado del Pago')
plt.show()

corr_cols = ['limit_bal', 'age'] + [c for c in df.columns if 'bill_amt' in c] + [c for c in df.columns if 'pay_amt' in c] + ['default_payment_next_month']
correlation_matrix = df[corr_cols].corr()

plt.figure(figsize=(12, 10))
sns.heatmap(correlation_matrix, cmap='viridis', annot=False)
plt.title('Matriz de Correlación', fontsize=16)
plt.show()

num_cols = ['limit_bal'] + [c for c in df.columns if c.startswith('bill_amt_')] + [c for c in df.columns if c.startswith('pay_amt_')]

print("\n--- Cuantiles Extremos de Variables Financieras ---")
v_q = df[num_cols].quantile([0, 0.01, 0.25, 0.5, 0.75, 0.99, 1]).T
display(v_q)

# Boxplots agrupados para una visualización compacta.
fig, axes = plt.subplots(3, 5, figsize=(20, 12))
axes = axes.flatten()

for i, col in enumerate(num_cols):
    sns.boxplot(x=df[col], ax=axes[i], color='lightblue')
    axes[i].set_title(col, fontsize=10)
    axes[i].set_xlabel('')

# Ocultar ejes sobrantes si el número de columnas no es múltiplo de 5.
for j in range(i + 1, len(axes)):
    axes[j].axis('off')

plt.suptitle('Detección de Valores Atípicos en Variables Financieras', fontsize=18)
plt.tight_layout(rect=[0, 0.03, 1, 0.97])
plt.show()

# Análisis temporal agregado por mes.
bill_cols = [c for c in df.columns if c.startswith('bill_amt_')]
pay_cols  = [c for c in df.columns if c.startswith('pay_amt_')]
month_names = ['Sept', 'Aug', 'July', 'June', 'May', 'April']

df_ts = pd.DataFrame({
    'facturacion': df[bill_cols].sum().values,
    'pagos'      : df[pay_cols].sum().values
}, index=month_names)

df_ts.plot(marker='o', figsize=(10, 5))
plt.title('Facturación Total vs. Pagos Totales por Mes')
plt.xlabel('Mes (2005)')
plt.ylabel('Monto Total (NT$)')
plt.grid(True, which='both', linestyle='--')
plt.show()

try:
    processed_data_path = project_root / "data" / "processed"
    processed_data_path.mkdir(parents=True, exist_ok=True)
    
    # Se eliminan las columnas de etiquetas antes de guardar.
    df_to_save = df.drop(columns=['sex_label', 'education_label', 'marriage_label'])
    
    file_path = processed_data_path / "credit_card_clients_clean.csv"
    df_to_save.to_csv(file_path, index=False)
    
    print(f"\nDataFrame limpio guardado exitosamente en: {file_path}")

except Exception as e:
    print(f"\nOcurrió un error al guardar el archivo: {e}")

try:
    project_root = Path.cwd().parent
    processed_data_path = project_root / "data" / "processed"
    
    processed_data_path.mkdir(parents=True, exist_ok=True)
    
    file_path = processed_data_path / "credit_card_clients_clean.csv"
    df_clean.to_csv(file_path, index=False)
    
    print(f"\ DataFrame limpio guardado exitosamente en: {file_path}")

except Exception as e:
    print(f"\ Ocurrió un error al guardar el archivo: {e}")

# --- Código extraído de: 04_Modelado_Regresion.ipynb ---
import pandas as pd
import numpy as np
from pathlib import Path
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression, HuberRegressor
from sklearn.ensemble import RandomForestRegressor
from xgboost import XGBRegressor
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
from sklearn.base import clone

sns.set_style("whitegrid")
plt.rcParams['figure.figsize'] = (10, 6)

try:
    project_root = Path.cwd().parent
    features_path = project_root / "data" / "processed" / "features_regresion.csv"
    df_reg = pd.read_csv(features_path)
    print(f"DataFrame para regresión cargado desde: {features_path}")
except FileNotFoundError:
    print(f"Error: No se encontró el archivo en: {features_path}")
    df_reg = None

def train_and_evaluate_regressor(X_train, y_train, X_test, y_test, model, model_name, use_log_transform=False):
    """
    Entrena un modelo de regresión, realiza predicciones y devuelve las métricas y artefactos.
    """
    # Guardar los nombres de las características para el análisis posterior
    feature_names = X_train.columns.tolist()
    
    # Aplicar transformación logarítmica si se especifica
    y_train_target = y_train
    if use_log_transform:
        y_train_target = np.log1p(y_train)

    # Entrenamiento
    model.fit(X_train, y_train_target)
    
    # Predicción
    y_pred = model.predict(X_test)
    
    # Transformación inversa si es necesario
    if use_log_transform:
        y_pred = np.expm1(y_pred)
    
    # Cálculo de métricas
    r2 = r2_score(y_test, y_pred)
    rmse = np.sqrt(mean_squared_error(y_test, y_pred))
    mae = mean_absolute_error(y_test, y_pred)
    
    transform_label = "Con Log" if use_log_transform else "Sin Log"
    print(f"\n--- Resultados para {model_name} ({transform_label}) ---")
    print(f"R² Score: {r2:.4f}")
    print(f"RMSE: {rmse:.4f}")
    print(f"MAE: {mae:.4f}")
    
    metrics = {
        'Model': model_name,
        'Transformation': transform_label,
        'R2 Score': r2,
        'RMSE': rmse,
        'MAE': mae,
        'artifacts': {
            'model': model,
            'feature_names': feature_names
        }
    }
    
    return metrics, y_pred

if df_reg is not None:
    # 1. División Temporal de Datos
    df_reg_sorted = df_reg.sort_values('ID').reset_index(drop=True)
    split_point = int(len(df_reg_sorted) * 0.8)
    train_df = df_reg_sorted.iloc[:split_point]
    test_df = df_reg_sorted.iloc[split_point:]

    # 2. Definir X e y, excluyendo el ID de las características
    target_col = 'pay_amt_june'
    features_to_use = df_reg.select_dtypes(include=np.number).columns.drop([target_col, 'ID'], errors='ignore').tolist()
    
    X_train = train_df[features_to_use]
    y_train = train_df[target_col]
    X_test = test_df[features_to_use]
    y_test = test_df[target_col]

    # 3. Escalado de Características (se pasa un DataFrame con nombres de columnas)
    scaler = StandardScaler()
    X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train), columns=features_to_use)
    X_test_scaled = pd.DataFrame(scaler.transform(X_test), columns=features_to_use)

    # 4. Definir Prototipos de Modelos
    model_prototypes = {
        "Regresión Lineal": LinearRegression(),
        "Regresión de Huber": HuberRegressor(max_iter=1000),
        "Random Forest": RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1),
        "XGBoost": XGBRegressor(objective='reg:squarederror', random_state=42, n_jobs=-1)
    }
    
    results = []
    predictions = {}

    # 5. Iterar, Clonar y Entrenar
    for name, model_proto in model_prototypes.items():
        # Experimento SIN transformación logarítmica
        model_no_log = clone(model_proto)
        metrics_no_log, y_pred_no_log = train_and_evaluate_regressor(X_train_scaled, y_train, X_test_scaled, y_test, model_no_log, name, use_log_transform=False)
        results.append(metrics_no_log)
        predictions[f"{name} (Sin Log)"] = y_pred_no_log
        
        # Experimento CON transformación logarítmica
        model_log = clone(model_proto)
        metrics_log, y_pred_log = train_and_evaluate_regressor(X_train_scaled, y_train, X_test_scaled, y_test, model_log, name, use_log_transform=True)
        results.append(metrics_log)
        predictions[f"{name} (Con Log)"] = y_pred_log

    # 6. Tabla Comparativa de Resultados
    results_df = pd.DataFrame(results).sort_values(by='R2 Score', ascending=False)
    
    print("\n--- Tabla Comparativa de Rendimiento de Modelos de Regresión ---")
    display(results_df.drop(columns=['artifacts']))

if 'results_df' in locals() and not results_df.empty:
    best_row = results_df.loc[results_df['R2 Score'].idxmax()]
    best_model_name = best_row['Model']
    best_transform_label = best_row['Transformation']
    best_prediction_key = f"{best_model_name} ({best_transform_label})"
    
    print(f"\n--- Análisis Profundo del Mejor Modelo: {best_model_name} ({best_transform_label}) ---")
    
    # 1. Gráfico de Predicciones vs. Valores Reales
    plt.figure(figsize=(8, 8))
    sns.scatterplot(x=y_test, y=predictions[best_prediction_key], alpha=0.5)
    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], '--r', linewidth=2)
    plt.title(f'Valores Reales vs. Predicciones ({best_model_name} - {best_transform_label})')
    plt.xlabel('Valores Reales (pay_amt_june)')
    plt.ylabel('Predicciones')
    plt.show()
    
    # 2. Importancia de Características (usando artefactos)
    artifacts = best_row['artifacts']
    best_model_instance = artifacts['model']
    feature_names = artifacts['feature_names']

    if hasattr(best_model_instance, 'feature_importances_'):
        importances = best_model_instance.feature_importances_
        
        feature_importance_df = pd.DataFrame({'feature': feature_names, 'importance': importances})
        feature_importance_df = feature_importance_df.sort_values(by='importance', ascending=False)
        
        plt.figure(figsize=(10, 10))
        sns.barplot(x='importance', y='feature', data=feature_importance_df.head(20), palette='viridis')
        plt.title(f'Top 20 Características más Importantes ({best_model_name})')
        plt.xlabel('Importancia')
        plt.ylabel('Característica')
        plt.tight_layout()
        plt.show()

# --- Código extraído de: 02_Ingenieria_de_Caracteristicas.ipynb ---
import pandas as pd
import numpy as np
from pathlib import Path
from statsmodels.stats.outliers_influence import variance_inflation_factor
from statsmodels.tools.tools import add_constant

try:
    project_root = Path.cwd().parent
    clean_data_path = project_root / "data" / "processed" / "credit_card_clients_clean.csv"
    df = pd.read_csv(clean_data_path)
    print(f"DataFrame limpio cargado desde: {clean_data_path}")
except FileNotFoundError:
    print(f"Error: No se encontró el archivo: {clean_data_path}")
    df = None

# --- 1. Corrección de Fuga de Datos y Nueva Ingeniería de Características ---

# Copiamos el dataframe limpio para empezar
df_features = df.copy()

# Definir grupos de columnas HISTÓRICAS para evitar data leakage
# Usamos datos de Agosto hacia atrás para predecir el default del mes siguiente (Octubre)
bill_amt_cols_hist = ['bill_amt_aug', 'bill_amt_july', 'bill_amt_june', 'bill_amt_may', 'bill_amt_april']
pay_amt_cols_hist = ['pay_amt_aug', 'pay_amt_july', 'pay_amt_june', 'pay_amt_may', 'pay_amt_april']
pay_status_cols_hist = ['pay_aug', 'pay_july', 'pay_june', 'pay_may', 'pay_april']

# -- A. Ratios de Salud Financiera (Corregido) --
epsilon = 1e-6
# Tasa de utilización del mes más reciente (Septiembre)
df_features['utilization_sept'] = df_features['bill_amt_sept'] / (df_features['limit_bal'] + epsilon)

# -- B. Agregados de Historial Financiero (Corregido) --
df_features['bill_amt_avg_hist'] = df_features[bill_amt_cols_hist].mean(axis=1)
df_features['bill_amt_std_hist'] = df_features[bill_amt_cols_hist].std(axis=1)
df_features['pay_amt_avg_hist'] = df_features[pay_amt_cols_hist].mean(axis=1)

# -- C. Características de Tendencia (Corregido) --
# Se calcula la pendiente sobre datos históricos para observar tendencias
def calculate_slope(row, cols):
    y = row[cols].values.astype(float)
    x = np.array(range(len(y)))
    slope, _ = np.polyfit(x, y, 1)
    return slope

# Aplicamos la función sobre las columnas históricas en orden cronológico (de más antiguo a más reciente)
df_features['bill_amt_slope'] = df_features.apply(lambda row: calculate_slope(row, bill_amt_cols_hist[::-1]), axis=1)
df_features['pay_status_slope'] = df_features.apply(lambda row: calculate_slope(row, pay_status_cols_hist[::-1]), axis=1)

# -- D. Indicadores de Comportamiento de Pago (Corregido) --
df_features['pay_status_avg_hist'] = df_features[pay_status_cols_hist].mean(axis=1)
df_features['months_with_delay_hist'] = (df_features[pay_status_cols_hist] > 0).sum(axis=1)

# -- E. Nuevas Características Propuestas --

# Volatilidad de pagos: Un cliente con pagos muy irregulares puede ser riesgoso
all_pay_amt_cols = ['pay_amt_sept'] + pay_amt_cols_hist
df_features['payment_volatility'] = df_features[all_pay_amt_cols].std(axis=1) / (df_features[all_pay_amt_cols].mean(axis=1) + epsilon)

# Proxy de relación deuda-ingreso: Compara la deuda promedio con el límite total
df_features['debt_to_limit_ratio'] = df_features['bill_amt_avg_hist'] / (df_features['limit_bal'] + epsilon)

# Reemplazar valores infinitos o NaN que puedan surgir de las divisiones
df_features.replace([np.inf, -np.inf], np.nan, inplace=True)
df_features.fillna(0, inplace=True) # O una estrategia de imputación más sofisticada

print("--- Ingeniería de Características Mejorada (sin fuga de datos) ---")
display(df_features.head())

if df is not None:
    # Se seleccionan solo las columnas numéricas para el cálculo de VIF.
    numeric_cols_for_vif = df_features.select_dtypes(include=np.number).drop(columns=['ID', 'default_payment_next_month'])
    
    # Se añade una constante para el cálculo correcto del VIF.
    X_vif = add_constant(numeric_cols_for_vif)
    
    vif_data = pd.DataFrame()
    vif_data["feature"] = X_vif.columns
    vif_data["VIF"] = [variance_inflation_factor(X_vif.values, i) for i in range(X_vif.shape[1])]
    
    print("\n--- Análisis de Factor de Inflación de la Varianza (VIF) ---")
    # Se muestran las 15 variables con mayor VIF, que son las más problemáticas.
    display(vif_data.sort_values(by='VIF', ascending=False).head(15))

if df is not None:
    features_reducidas = [
        'ID',
        # Demográficas y Límite de Crédito
        'limit_bal', 'sex', 'education', 'marriage', 'age',
        # Comportamiento de Pago (las más predictivas)
        'pay_sept', 'pay_status_slope',
        # Salud Financiera (menos correlacionadas)
        'utilization_sept', 'bill_amt_std_hist', 'pay_amt_avg_hist',
        # Variable Objetivo
        'default_payment_next_month'
    ]
    df_reducido = df_features[features_reducidas]
    print("\n--- Dataset Reducido Creado ---")
    display(df_reducido.head())

if df is not None:
    df_features_reg = df.copy()
    
    # Se usan solo datos hasta junio para evitar fuga de información.
    bill_cols_reg = ['bill_amt_june', 'bill_amt_may', 'bill_amt_april']
    pay_cols_reg = ['pay_amt_may', 'pay_amt_april']
    
    # 6.1. Ratios y Agregados
    df_features_reg['utilization_june'] = df_features_reg['bill_amt_june'] / (df_features_reg['limit_bal'] + epsilon)
    df_features_reg['bill_amt_avg_3m'] = df_features_reg[bill_cols_reg].mean(axis=1)
    df_features_reg['pay_amt_avg_2m'] = df_features_reg[pay_cols_reg].mean(axis=1)

    print("\n--- Ingeniería de Características para Regresión completada ---")

if df is not None:
    try:
        processed_data_path = project_root / "data" / "processed"
        
        # Guardar dataset completo para clasificación
        path_clasificacion = processed_data_path / "features_clasificacion.csv"
        df_features.to_csv(path_clasificacion, index=False)
        print(f"\nDataset para clasificación guardado en: {path_clasificacion}")

        # Guardar dataset reducido para clasificación
        path_reducido = processed_data_path / "features_reducido_clasificacion.csv"
        df_reducido.to_csv(path_reducido, index=False)
        print(f"Dataset reducido para clasificación guardado en: {path_reducido}")
        
        # Guardar dataset para regresión
        path_regresion = processed_data_path / "features_regresion.csv"
        df_features_reg.to_csv(path_regresion, index=False)
        print(f"Dataset para regresión guardado en: {path_regresion}")

    except Exception as e:
        print(f"\nOcurrió un error al guardar los archivos: {e}")

