# --- C√≥digo extra√≠do de: 05_Modelo_Avanzado.ipynb ---
# Modelado Avanzado Mejorado - Credit Scoring
import pandas as pd
import numpy as np
from pathlib import Path
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
from datetime import datetime

# Preprocesamiento y Pipelines
from sklearn.model_selection import (train_test_split, StratifiedKFold, 
                                     TimeSeriesSplit, cross_val_score)
from sklearn.preprocessing import StandardScaler, PowerTransformer, RobustScaler
from sklearn.pipeline import Pipeline
from sklearn.base import BaseEstimator, TransformerMixin
from imblearn.pipeline import Pipeline as ImbPipeline
from imblearn.combine import SMOTEENN
from imblearn.over_sampling import ADASYN
from sklearn.calibration import CalibratedClassifierCV
from sklearn.ensemble import IsolationForest

# Modelos Avanzados
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor, HistGradientBoostingRegressor
try:
    from xgboost import XGBClassifier, XGBRegressor
    from lightgbm import LGBMClassifier, LGBMRegressor
    from catboost import CatBoostClassifier, CatBoostRegressor
    ADVANCED_LIBS_AVAILABLE = True
except ImportError:
    print("Advertencia: XGBoost, LightGBM o CatBoost no disponibles. Usando modelos base.")
    ADVANCED_LIBS_AVAILABLE = False

try:
    import optuna
    OPTUNA_AVAILABLE = True
except ImportError:
    print("Advertencia: Optuna no disponible. Usando RandomizedSearchCV.")
    from sklearn.model_selection import RandomizedSearchCV
    OPTUNA_AVAILABLE = False

# M√©tricas y Interpretabilidad
from sklearn.metrics import (classification_report, roc_auc_score, f1_score, 
                             precision_recall_curve, auc, r2_score, 
                             mean_squared_error, precision_score, average_precision_score)
import shap
from scipy.stats import ks_2samp
from scipy.stats.mstats import winsorize

warnings.filterwarnings('ignore', category=FutureWarning)
sns.set_style("whitegrid")
plt.rcParams['figure.figsize'] = (12, 7)
RANDOM_STATE = 42

class TemporalFeatureEngineer(BaseEstimator, TransformerMixin):
    """
    Transformador que crea caracter√≠sticas temporales evitando data leakage.
    Solo usa informaci√≥n hist√≥rica previa al punto de predicci√≥n.
    """
    def __init__(self, lookback_months=3, target_month='sept'):
        self.lookback_months = lookback_months
        self.target_month = target_month
        self.month_order = ['april', 'may', 'june', 'july', 'aug', 'sept']
        
    def fit(self, X, y=None):
        return self
    
    def transform(self, X):
        X_transformed = X.copy()
        
        # Obtener √≠ndice del mes objetivo
        target_idx = self.month_order.index(self.target_month)
        historical_months = self.month_order[:target_idx][-self.lookback_months:]
        
        # Solo usar meses hist√≥ricos para evitar data leakage
        pay_cols = [f'pay_amt_{month}' for month in historical_months]
        bill_cols = [f'bill_amt_{month}' for month in historical_months]
        pay_status_cols = [f'pay_{month}' for month in historical_months]
        
        # Caracter√≠sticas de tendencia temporal
        if len(pay_cols) >= 2:
            X_transformed['payment_trend'] = self._calculate_trend(X_transformed, pay_cols)
            X_transformed['bill_trend'] = self._calculate_trend(X_transformed, bill_cols)
            X_transformed['payment_volatility'] = X_transformed[pay_cols].std(axis=1)
            
        # Caracter√≠sticas de comportamiento de pago
        X_transformed['avg_payment_delay'] = X_transformed[pay_status_cols].mean(axis=1)
        X_transformed['months_with_delay'] = (X_transformed[pay_status_cols] > 0).sum(axis=1)
        X_transformed['consecutive_delays'] = self._calculate_consecutive_delays(X_transformed, pay_status_cols)
        
        # Ratios financieros
        epsilon = 1e-6
        recent_bill = f'bill_amt_{historical_months[-1]}'
        recent_pay = f'pay_amt_{historical_months[-1]}'
        
        X_transformed['utilization_recent'] = X_transformed[recent_bill] / (X_transformed['limit_bal'] + epsilon)
        X_transformed['payment_ratio_recent'] = X_transformed[recent_pay] / (X_transformed[recent_bill] + epsilon)
        X_transformed['debt_to_limit_ratio'] = X_transformed[bill_cols].mean(axis=1) / (X_transformed['limit_bal'] + epsilon)
        
        return X_transformed
    
    def _calculate_trend(self, df, cols):
        """Calcula la tendencia temporal usando regresi√≥n lineal simple."""
        trends = []
        for idx, row in df.iterrows():
            y_vals = row[cols].values.astype(float)
            if len(y_vals) > 1:
                x_vals = np.arange(len(y_vals))
                slope, _ = np.polyfit(x_vals, y_vals, 1)
                trends.append(slope)
            else:
                trends.append(0)
        return pd.Series(trends, index=df.index)
    
    def _calculate_consecutive_delays(self, df, pay_status_cols):
        """Calcula el n√∫mero m√°ximo de retrasos consecutivos."""
        consecutive_delays = []
        for idx, row in df.iterrows():
            delays = (row[pay_status_cols] > 0).astype(int).values
            max_consecutive = 0
            current_consecutive = 0
            
            for delay in delays:
                if delay == 1:
                    current_consecutive += 1
                    max_consecutive = max(max_consecutive, current_consecutive)
                else:
                    current_consecutive = 0
            
            consecutive_delays.append(max_consecutive)
        
        return pd.Series(consecutive_delays, index=df.index)

class AdaptiveOutlierHandler(BaseEstimator, TransformerMixin):
    """
    Maneja outliers de forma adaptativa usando Isolation Forest
    en lugar de winsorizaci√≥n fija.
    """
    def __init__(self, contamination=0.05, random_state=42):
        self.contamination = contamination
        self.random_state = random_state
        self.outlier_detectors = {}
        
    def fit(self, X, y=None):
        numeric_cols = X.select_dtypes(include=[np.number]).columns
        
        for col in numeric_cols:
            if X[col].std() > 0:  # Solo para columnas con variabilidad
                detector = IsolationForest(
                    contamination=self.contamination,
                    random_state=self.random_state
                )
                detector.fit(X[[col]])
                self.outlier_detectors[col] = detector
        
        return self
    
    def transform(self, X):
        X_transformed = X.copy()
        
        for col, detector in self.outlier_detectors.items():
            outlier_mask = detector.predict(X_transformed[[col]]) == -1
            if outlier_mask.sum() > 0:
                # Reemplazar outliers con percentiles
                p99 = X_transformed[col].quantile(0.99)
                p01 = X_transformed[col].quantile(0.01)
                
                X_transformed.loc[outlier_mask & (X_transformed[col] > p99), col] = p99
                X_transformed.loc[outlier_mask & (X_transformed[col] < p01), col] = p01
        
        return X_transformed

class BusinessMetricsCalculator:
    """
    Calcula m√©tricas orientadas al negocio bancario.
    """
    def __init__(self, cost_fp=100, benefit_tp=500, cost_fn=1000):
        self.cost_fp = cost_fp  # Costo de falso positivo (rechazar buen cliente)
        self.benefit_tp = benefit_tp  # Beneficio de verdadero positivo (identificar riesgo)
        self.cost_fn = cost_fn  # Costo de falso negativo (aprobar mal cliente)
    
    def calculate_profit_curve(self, y_true, y_pred_proba):
        """Calcula la curva de profit para diferentes thresholds."""
        thresholds = np.linspace(0.01, 0.99, 100)
        profits = []
        
        for threshold in thresholds:
            y_pred = (y_pred_proba >= threshold).astype(int)
            profit = self._calculate_profit(y_true, y_pred)
            profits.append(profit)
        
        optimal_idx = np.argmax(profits)
        return thresholds, profits, thresholds[optimal_idx], profits[optimal_idx]
    
    def _calculate_profit(self, y_true, y_pred):
        """Calcula el profit total basado en la matriz de confusi√≥n."""
        tp = ((y_true == 1) & (y_pred == 1)).sum()
        fp = ((y_true == 0) & (y_pred == 1)).sum()
        fn = ((y_true == 1) & (y_pred == 0)).sum()
        
        profit = (tp * self.benefit_tp) - (fp * self.cost_fp) - (fn * self.cost_fn)
        return profit
    
    def precision_at_k(self, y_true, y_pred_proba, k_values=[0.1, 0.2, 0.3]):
        """Calcula precision at k para diferentes percentiles de riesgo."""
        metrics = {}
        
        for k in k_values:
            threshold = np.percentile(y_pred_proba, (1-k)*100)
            high_risk_mask = y_pred_proba >= threshold
            
            if high_risk_mask.sum() > 0:
                precision_k = y_true[high_risk_mask].mean()
                metrics[f'precision_at_{k}'] = precision_k
            else:
                metrics[f'precision_at_{k}'] = 0
        
        return metrics

def create_ensemble_model():
    """Crea un modelo ensemble diversificado."""
    models = []
    
    if ADVANCED_LIBS_AVAILABLE:
        # Modelo basado en √°rboles con regularizaci√≥n
        lgbm = LGBMClassifier(
            n_estimators=200,
            max_depth=6,
            learning_rate=0.1,
            subsample=0.8,
            colsample_bytree=0.8,
            reg_alpha=0.1,
            reg_lambda=0.1,
            random_state=RANDOM_STATE,
            verbose=-1
        )
        
        # Modelo con manejo autom√°tico de categ√≥ricas
        catboost = CatBoostClassifier(
            iterations=200,
            depth=6,
            learning_rate=0.1,
            l2_leaf_reg=3,
            random_state=RANDOM_STATE,
            verbose=False
        )
        
        models.extend([
            ('lgbm', lgbm),
            ('catboost', catboost)
        ])
    
    # Modelo lineal para diversidad
    log_reg = LogisticRegression(
        C=0.1,
        penalty='elasticnet',
        l1_ratio=0.5,
        solver='saga',
        max_iter=1000,
        random_state=RANDOM_STATE
    )
    
    # Random Forest con par√°metros optimizados
    rf = RandomForestClassifier(
        n_estimators=200,
        max_depth=15,
        min_samples_split=10,
        min_samples_leaf=5,
        max_features='sqrt',
        class_weight='balanced_subsample',
        random_state=RANDOM_STATE,
        n_jobs=-1
    )
    
    models.extend([
        ('logistic', log_reg),
        ('random_forest', rf)
    ])
    
    return models

try:
    project_root = Path.cwd().parent
    processed_path = project_root / "data" / "processed"
    
    df_full = pd.read_csv(processed_path / "features_clasificacion.csv")
    df_reg = pd.read_csv(processed_path / "features_regresion.csv")
    
    print("Datasets cargados exitosamente.")
    print(f"Forma del dataset de clasificaci√≥n: {df_full.shape}")
    print(f"Forma del dataset de regresi√≥n: {df_reg.shape}")

except FileNotFoundError as e:
    print(f"Error: No se encontr√≥ el archivo. Verifica la ruta: {e}")
    df_full = None
    df_reg = None

if df_full is not None:
    print("\n" + "="*70)
    print("MODELADO DE CLASIFICACI√ìN AVANZADO")
    print("="*70)
    
    # Preparaci√≥n de datos con validaci√≥n temporal
    X = df_full.select_dtypes(include=np.number).drop(columns=['default_payment_next_month', 'ID'])
    y = df_full['default_payment_next_month']
    
    # Simular orden temporal basado en ID (asumiendo orden cronol√≥gico)
    df_full_sorted = df_full.sort_values('ID').reset_index(drop=True)
    split_point = int(len(df_full_sorted) * 0.8)
    
    # Divisi√≥n temporal: 80% entrenamiento, 20% prueba
    X_train = df_full_sorted.iloc[:split_point].select_dtypes(include=np.number).drop(columns=['default_payment_next_month', 'ID'])
    X_test = df_full_sorted.iloc[split_point:].select_dtypes(include=np.number).drop(columns=['default_payment_next_month', 'ID'])
    y_train = df_full_sorted.iloc[:split_point]['default_payment_next_month']
    y_test = df_full_sorted.iloc[split_point:]['default_payment_next_month']
    
    print(f"Divisi√≥n temporal - Entrenamiento: {X_train.shape[0]}, Prueba: {X_test.shape[0]}")
    
    # Pipeline mejorado con transformadores custom
    feature_engineer = TemporalFeatureEngineer(lookback_months=3, target_month='sept')
    outlier_handler = AdaptiveOutlierHandler(contamination=0.05)
    
    # Crear ensemble diversificado
    ensemble_models = create_ensemble_model()
    best_models = {}
    
    print("\n--- Entrenando Modelos del Ensemble ---")
    
    for name, base_model in ensemble_models:
        print(f"\nEntrenando {name}...")
        
        # Pipeline completo para cada modelo
        pipeline = ImbPipeline([
            ('feature_engineer', feature_engineer),
            ('outlier_handler', outlier_handler),
            ('sampler', ADASYN(random_state=RANDOM_STATE)),  # ADASYN en lugar de SMOTE
            ('scaler', RobustScaler()),  # RobustScaler para mejor manejo de outliers
            ('classifier', base_model)
        ])
        
        # Entrenamiento con validaci√≥n cruzada temporal
        test_size_int = int(len(X_train) * 0.15) 
        tscv = TimeSeriesSplit(n_splits=5, test_size=test_size_int)

        cv_scores = cross_val_score(pipeline, X_train, y_train, cv=tscv, 
                                scoring='f1', n_jobs=-1)
        
        print(f"F1-Score CV: {cv_scores.mean():.4f} (+/- {cv_scores.std()*2:.4f})")
        
        # Entrenamiento final
        pipeline.fit(X_train, y_train)
        best_models[name] = pipeline
    
    # Selecci√≥n del mejor modelo basado en m√©tricas de negocio
    print("\n--- Evaluaci√≥n y Selecci√≥n del Mejor Modelo ---")
    business_calc = BusinessMetricsCalculator()
    model_scores = {}
    
    for name, model in best_models.items():
        y_pred_proba = model.predict_proba(X_test)[:, 1]
        
        # M√©tricas t√©cnicas
        roc_auc = roc_auc_score(y_test, y_pred_proba)
        avg_precision = average_precision_score(y_test, y_pred_proba)
        
        # M√©tricas de negocio
        _, profits, optimal_threshold, max_profit = business_calc.calculate_profit_curve(y_test, y_pred_proba)
        precision_k = business_calc.precision_at_k(y_test, y_pred_proba)
        
        model_scores[name] = {
            'roc_auc': roc_auc,
            'avg_precision': avg_precision,
            'max_profit': max_profit,
            'optimal_threshold': optimal_threshold,
            **precision_k
        }
        
        print(f"\n{name.upper()}:")
        print(f"  ROC-AUC: {roc_auc:.4f}")
        print(f"  Avg Precision: {avg_precision:.4f}")
        print(f"  Max Profit: ${max_profit:,.0f}")
        print(f"  Optimal Threshold: {optimal_threshold:.3f}")
        print(f"  Precision@10%: {precision_k.get('precision_at_0.1', 0):.3f}")
    
    # Seleccionar mejor modelo basado en profit
    best_model_name = max(model_scores.keys(), 
                         key=lambda x: model_scores[x]['max_profit'])
    best_model = best_models[best_model_name]
    best_threshold = model_scores[best_model_name]['optimal_threshold']
    
    print(f"\nüèÜ MEJOR MODELO: {best_model_name.upper()}")
    print(f"Threshold √≥ptimo: {best_threshold:.3f}")
    
    # Calibraci√≥n de probabilidades del mejor modelo
    print("\n--- Calibrando Probabilidades ---")
    
    # Extraer el clasificador del pipeline para calibraci√≥n
    X_train_transformed = best_model[:-1].transform(X_train)
    base_classifier = best_model.named_steps['classifier']
    
    # Calibraci√≥n con validaci√≥n cruzada
    calibrated_clf = CalibratedClassifierCV(
        base_classifier, 
        method='isotonic', 
        cv=3
    )
    calibrated_clf.fit(X_train_transformed, y_train)
    
    # Predicciones finales calibradas
    X_test_transformed = best_model[:-1].transform(X_test)
    y_pred_proba_calibrated = calibrated_clf.predict_proba(X_test_transformed)[:, 1]
    y_pred_final = (y_pred_proba_calibrated >= best_threshold).astype(int)
    
    # Evaluaci√≥n final
    print("\n--- EVALUACI√ìN FINAL ---")
    print(classification_report(y_test, y_pred_final, 
                              target_names=['No Default', 'Default']))
    
    final_roc_auc = roc_auc_score(y_test, y_pred_proba_calibrated)
    print(f"\nROC-AUC Final (Calibrado): {final_roc_auc:.4f}")
    
    # Visualizaciones
    fig, axes = plt.subplots(2, 2, figsize=(15, 12))
    
    # 1. Curva Precision-Recall
    precision, recall, _ = precision_recall_curve(y_test, y_pred_proba_calibrated)
    pr_auc = auc(recall, precision)
    
    axes[0,0].plot(recall, precision, label=f'PR AUC = {pr_auc:.3f}')
    axes[0,0].set_xlabel('Recall')
    axes[0,0].set_ylabel('Precision')
    axes[0,0].set_title('Curva Precision-Recall')
    axes[0,0].legend()
    axes[0,0].grid(True)
    
    # 2. Curva de Profit
    thresholds, profits, opt_thresh, max_profit = business_calc.calculate_profit_curve(
        y_test, y_pred_proba_calibrated)
    
    axes[0,1].plot(thresholds, profits)
    axes[0,1].axvline(opt_thresh, color='red', linestyle='--', 
                     label=f'√ìptimo: {opt_thresh:.3f}')
    axes[0,1].set_xlabel('Threshold')
    axes[0,1].set_ylabel('Profit ($)')
    axes[0,1].set_title('Curva de Profit')
    axes[0,1].legend()
    axes[0,1].grid(True)
    
    # 3. Distribuci√≥n de Probabilidades
    axes[1,0].hist(y_pred_proba_calibrated[y_test == 0], alpha=0.7, 
                  label='No Default', bins=30)
    axes[1,0].hist(y_pred_proba_calibrated[y_test == 1], alpha=0.7, 
                  label='Default', bins=30)
    axes[1,0].axvline(best_threshold, color='red', linestyle='--', 
                     label=f'Threshold: {best_threshold:.3f}')
    axes[1,0].set_xlabel('Probabilidad Predicha')
    axes[1,0].set_ylabel('Frecuencia')
    axes[1,0].set_title('Distribuci√≥n de Probabilidades')
    axes[1,0].legend()
    
    # 4. Matriz de Confusi√≥n Normalizada
    from sklearn.metrics import confusion_matrix
    cm = confusion_matrix(y_test, y_pred_final, normalize='true')
    sns.heatmap(cm, annot=True, fmt='.3f', cmap='Blues', ax=axes[1,1])
    axes[1,1].set_title('Matriz de Confusi√≥n (Normalizada)')
    axes[1,1].set_ylabel('Valor Real')
    axes[1,1].set_xlabel('Predicci√≥n')
    
    plt.tight_layout()
    plt.show()
    
    # An√°lisis de Interpretabilidad con SHAP
    print("\n--- An√°lisis de Interpretabilidad ---")

    # Usar una muestra un poco m√°s grande para el fondo del explainer si es posible
    X_train_sample = X_train_transformed[:500]
    X_test_for_shap = X_test_transformed[:200]  # Muestra para eficiencia en predicci√≥n

    try:
        # 1. Crear el Explainer de SHAP
        if 'tree' in str(type(base_classifier)).lower():
            explainer = shap.TreeExplainer(base_classifier)
        else:
            # Usamos un background dataset para aproximar las expectativas
            explainer = shap.Explainer(base_classifier.predict_proba, X_train_sample)
        
        # 2. Calcular los valores SHAP
        # Para modelos de probabilidad, accedemos a la salida de la clase positiva (Default)
        shap_values = explainer(X_test_for_shap)
        
        # Asegurarnos de que estamos usando los valores de la clase 'Default'
        # La nueva API de SHAP a menudo devuelve un objeto explainer con .values y .base_values
        if isinstance(shap_values, list): # Para la API antigua
            shap_values_positive = shap_values[1]
        else: # Para la nueva API
            shap_values_positive = shap_values.values[:,:,1]

        # Obtener nombres de caracter√≠sticas (con el fallback que ya ten√≠as)
        try:
            feature_names = best_model[:-1].get_feature_names_out()
        except Exception:
            print("Advertencia: No se pudieron obtener los nombres de las caracter√≠sticas del pipeline.")
            print("Se usar√°n nombres gen√©ricos.")
            feature_names = [f"feature_{i}" for i in range(X_test_for_shap.shape[1])]

        # Convertir los datos de prueba a un DataFrame con los nombres correctos
        X_test_shap_df = pd.DataFrame(X_test_for_shap, columns=feature_names)

        # 3. Gr√°fico de Importancia (Corregido)
        # Usamos el summary plot por defecto (tipo 'dot'), que es m√°s robusto e informativo.
        print("\nImportancia de Caracter√≠sticas (SHAP Summary Plot):")
        shap.summary_plot(shap_values_positive, X_test_shap_df, max_display=15)

        # 4. Gr√°fico de Dependencia/Interacci√≥n (Corregido y A√±adido)
        # Este gr√°fico muestra c√≥mo una caracter√≠stica impacta la predicci√≥n
        # y c√≥mo interact√∫a con otra caracter√≠stica (elegida autom√°ticamente por SHAP).
        print("\nGr√°fico de Dependencia (ej. para la caracter√≠stica m√°s importante):")
        
        # Obtener la caracter√≠stica m√°s importante para mostrar un ejemplo
        most_important_feature = X_test_shap_df.columns[np.argsort(np.abs(shap_values_positive).mean(0))[-1]]
        
        fig, ax = plt.subplots() # Crear figura para tener m√°s control
        shap.dependence_plot(most_important_feature, shap_values_positive, X_test_shap_df, ax=ax)
        plt.tight_layout()  # <-- CLAVE: Ajusta el gr√°fico para que no se corte
        plt.show()          # <-- CLAVE: Muestra el gr√°fico ajustado

    except Exception as e:
        print(f"Error en an√°lisis SHAP: {e}")
        print("Continuando sin an√°lisis de interpretabilidad...")

if df_reg is not None:
    print("\n" + "="*70)
    print("MODELADO DE REGRESI√ìN AVANZADO (HURDLE MODEL)")
    print("="*70)
    
    # Preparaci√≥n de datos
    X_reg = df_reg.select_dtypes(include=np.number).drop(columns=['pay_amt_june', 'ID'])
    y_reg = df_reg['pay_amt_june']
    
    # Crear variable binaria para el hurdle (¬øpaga algo?)
    y_hurdle_binary = (y_reg > 0).astype(int)
    
    # Divisi√≥n temporal
    df_reg_sorted = df_reg.sort_values('ID').reset_index(drop=True)
    split_point_reg = int(len(df_reg_sorted) * 0.8)
    
    X_train_reg = df_reg_sorted.iloc[:split_point_reg].select_dtypes(include=np.number).drop(columns=['pay_amt_june', 'ID'])
    X_test_reg = df_reg_sorted.iloc[split_point_reg:].select_dtypes(include=np.number).drop(columns=['pay_amt_june', 'ID'])
    y_train_reg = df_reg_sorted.iloc[:split_point_reg]['pay_amt_june']
    y_test_reg = df_reg_sorted.iloc[split_point_reg:]['pay_amt_june']
    y_train_binary = (y_train_reg > 0).astype(int)
    y_test_binary = (y_test_reg > 0).astype(int)
    
    # Datos solo para clientes que pagan (monto > 0)
    positive_mask_train = y_train_reg > 0
    positive_mask_test = y_test_reg > 0
    
    X_train_positive = X_train_reg[positive_mask_train]
    X_test_positive = X_test_reg[positive_mask_test]
    y_train_positive = y_train_reg[positive_mask_train]
    y_test_positive = y_test_reg[positive_mask_test]
    
    print(f"Clientes que pagan en entrenamiento: {positive_mask_train.sum()}")
    print(f"Clientes que pagan en prueba: {positive_mask_test.sum()}")
    
    # ETAPA 1: Modelo de Clasificaci√≥n (Hurdle)
    print("\n--- ETAPA 1: Modelo Hurdle (¬øPagar√° algo?) ---")
    
    hurdle_pipeline = ImbPipeline([
        ('outlier_handler', AdaptiveOutlierHandler(contamination=0.03)),
        ('sampler', ADASYN(random_state=RANDOM_STATE)),
        ('scaler', RobustScaler()),
        ('classifier', LGBMClassifier(
            n_estimators=150,
            max_depth=6,
            learning_rate=0.1,
            random_state=RANDOM_STATE,
            verbose=-1
        ) if ADVANCED_LIBS_AVAILABLE else RandomForestClassifier(
            n_estimators=150, 
            max_depth=10, 
            random_state=RANDOM_STATE, 
            n_jobs=-1
        ))
    ])
    
    # Entrenamiento del modelo hurdle
    hurdle_pipeline.fit(X_train_reg, y_train_binary)
    
    # Evaluaci√≥n del hurdle
    y_pred_hurdle = hurdle_pipeline.predict(X_test_reg)
    y_pred_hurdle_proba = hurdle_pipeline.predict_proba(X_test_reg)[:, 1]
    
    hurdle_auc = roc_auc_score(y_test_binary, y_pred_hurdle_proba)
    print(f"ROC-AUC Modelo Hurdle: {hurdle_auc:.4f}")
    print(f"Precisi√≥n Hurdle: {(y_pred_hurdle == y_test_binary).mean():.4f}")
    
    # ETAPA 2: Modelo de Regresi√≥n (¬øCu√°nto pagar√°?)
    print("\n--- ETAPA 2: Modelo de Monto ---")
    
    # Pipeline para regresi√≥n con manejo mejorado de outliers
    regression_pipeline = Pipeline([
        ('outlier_handler', AdaptiveOutlierHandler(contamination=0.05)),
        ('scaler', RobustScaler()),
        ('transformer', PowerTransformer(method='yeo-johnson')),
        ('regressor', HistGradientBoostingRegressor(
            max_iter=200,
            max_depth=8,
            learning_rate=0.1,
            l2_regularization=0.1,
            random_state=RANDOM_STATE
        ))
    ])
    
    # Entrenamiento con validaci√≥n cruzada temporal
    test_size_reg_int = int(len(X_train_positive) * 0.2)
    tscv_reg = TimeSeriesSplit(n_splits=4, test_size=test_size_reg_int)

    rmse_scores = cross_val_score(
        regression_pipeline, 
        X_train_positive, 
        y_train_positive, 
        cv=tscv_reg, 
        scoring='neg_root_mean_squared_error',
        n_jobs=-1
    )
    
    print(f"RMSE CV: {-rmse_scores.mean():.2f} (+/- {rmse_scores.std()*2:.2f})")
    
    # Entrenamiento final
    regression_pipeline.fit(X_train_positive, y_train_positive)
    
    # COMBINACI√ìN FINAL DEL MODELO HURDLE
    print("\n--- EVALUACI√ìN FINAL DEL MODELO HURDLE COMBINADO ---")
    
    # Predicciones de la etapa 1 (¬øpagar√°?)
    will_pay_proba = hurdle_pipeline.predict_proba(X_test_reg)[:, 1]
    will_pay_pred = will_pay_proba > 0.5
    
    # Predicciones de la etapa 2 (¬øcu√°nto?)
    amount_pred = regression_pipeline.predict(X_test_reg)
    
    # Combinaci√≥n: monto predicho * probabilidad de pago
    final_predictions = amount_pred * will_pay_proba
    
    # M√©tricas de evaluaci√≥n
    r2_hurdle = r2_score(y_test_reg, final_predictions)
    rmse_hurdle = np.sqrt(mean_squared_error(y_test_reg, final_predictions))
    mae_hurdle = np.mean(np.abs(y_test_reg - final_predictions))
    
    # M√©tricas adicionales para evaluaci√≥n de negocio
    def mean_absolute_percentage_error(y_true, y_pred):
        return np.mean(np.abs((y_true - y_pred) / np.maximum(y_true, 1))) * 100
    
    mape_hurdle = mean_absolute_percentage_error(y_test_reg, final_predictions)
    
    # Evaluar solo en clientes que realmente pagan
    if positive_mask_test.sum() > 0:
        r2_positive_only = r2_score(y_test_positive, final_predictions[positive_mask_test])
        rmse_positive_only = np.sqrt(mean_squared_error(y_test_positive, final_predictions[positive_mask_test]))
    else:
        r2_positive_only = rmse_positive_only = 0
    
    print(f"R¬≤ Score (Todos): {r2_hurdle:.4f}")
    print(f"RMSE (Todos): {rmse_hurdle:.2f}")
    print(f"MAE (Todos): {mae_hurdle:.2f}")
    print(f"MAPE (Todos): {mape_hurdle:.2f}%")
    print(f"R¬≤ Score (Solo pagadores): {r2_positive_only:.4f}")
    print(f"RMSE (Solo pagadores): {rmse_positive_only:.2f}")
    
    # Visualizaciones del modelo de regresi√≥n
    fig, axes = plt.subplots(2, 2, figsize=(15, 12))
    
    # 1. Predicciones vs Valores Reales (Todos)
    axes[0,0].scatter(y_test_reg, final_predictions, alpha=0.6, s=20)
    axes[0,0].plot([y_test_reg.min(), y_test_reg.max()], 
                   [y_test_reg.min(), y_test_reg.max()], '--r', linewidth=2)
    axes[0,0].set_xlabel('Valores Reales')
    axes[0,0].set_ylabel('Predicciones')
    axes[0,0].set_title(f'Predicciones vs Reales (Todos)\nR¬≤ = {r2_hurdle:.3f}')
    axes[0,0].grid(True, alpha=0.3)
    
    # 2. Predicciones vs Valores Reales (Solo pagadores)
    if positive_mask_test.sum() > 0:
        axes[0,1].scatter(y_test_positive, final_predictions[positive_mask_test], 
                         alpha=0.6, s=20, color='green')
        axes[0,1].plot([y_test_positive.min(), y_test_positive.max()], 
                       [y_test_positive.min(), y_test_positive.max()], '--r', linewidth=2)
        axes[0,1].set_xlabel('Valores Reales')
        axes[0,1].set_ylabel('Predicciones')
        axes[0,1].set_title(f'Predicciones vs Reales (Pagadores)\nR¬≤ = {r2_positive_only:.3f}')
        axes[0,1].grid(True, alpha=0.3)
    
    # 3. Distribuci√≥n de Residuos
    residuals = y_test_reg - final_predictions
    axes[1,0].hist(residuals, bins=50, alpha=0.7, edgecolor='black')
    axes[1,0].axvline(residuals.mean(), color='red', linestyle='--', 
                     label=f'Media: {residuals.mean():.2f}')
    axes[1,0].set_xlabel('Residuos')
    axes[1,0].set_ylabel('Frecuencia')
    axes[1,0].set_title('Distribuci√≥n de Residuos')
    axes[1,0].legend()
    axes[1,0].grid(True, alpha=0.3)
    
    # 4. Q-Q Plot de Residuos
    from scipy import stats
    stats.probplot(residuals, dist="norm", plot=axes[1,1])
    axes[1,1].set_title('Q-Q Plot de Residuos')
    axes[1,1].grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.show()
    
    # An√°lisis de Importancia de Caracter√≠sticas
    print("\n--- An√°lisis de Importancia de Caracter√≠sticas ---")
    
    # Importancia del modelo hurdle (clasificaci√≥n)
    if hasattr(hurdle_pipeline.named_steps['classifier'], 'feature_importances_'):
        # Obtener datos transformados para obtener nombres correctos
        X_transformed_sample = hurdle_pipeline[:-1].transform(X_train_reg[:100])
        hurdle_importances = hurdle_pipeline.named_steps['classifier'].feature_importances_
        
        # Crear DataFrame de importancias del hurdle
        try:
            hurdle_feature_names = hurdle_pipeline[:-1].get_feature_names_out()
        except Exception:
            print("Advertencia: No se pudieron obtener los nombres de las caracter√≠sticas del pipeline del hurdle.")
            hurdle_feature_names = [f'feature_{i}' for i in range(len(hurdle_importances))]

        hurdle_importance_df = pd.DataFrame({
            'feature': hurdle_feature_names,
            'importance': hurdle_importances,
            'model': 'Hurdle (¬øPagar√°?)'
        }).sort_values('importance', ascending=False)

    print("Top 10 caracter√≠sticas m√°s importantes para el modelo Hurdle:")
    print(hurdle_importance_df.head(10))
    
    # Importancia del modelo de regresi√≥n
    if hasattr(regression_pipeline.named_steps['regressor'], 'feature_importances_'):
        reg_importances = regression_pipeline.named_steps['regressor'].feature_importances_
        
        try:
            reg_feature_names = regression_pipeline[:-1].get_feature_names_out()
        except Exception:
            print("Advertencia: No se pudieron obtener los nombres de las caracter√≠sticas del pipeline de regresi√≥n.")
            reg_feature_names = [f'feature_{i}' for i in range(len(reg_importances))]
            
        reg_importance_df = pd.DataFrame({
            'feature': reg_feature_names,
            'importance': reg_importances,
            'model': 'Regresi√≥n (¬øCu√°nto?)'
        }).sort_values('importance', ascending=False)
        
        print("\nTop 10 caracter√≠sticas m√°s importantes para el modelo de Regresi√≥n:")
        print(reg_importance_df.head(10))
        
        # Visualizaci√≥n comparativa de importancias
        if 'hurdle_importance_df' in locals():
            plt.figure(figsize=(15, 8))
            
            # Subplot para modelo hurdle
            plt.subplot(1, 2, 1)
            top_hurdle = hurdle_importance_df.head(15)
            plt.barh(range(len(top_hurdle)), top_hurdle['importance'])
            plt.yticks(range(len(top_hurdle)), top_hurdle['feature'])
            plt.xlabel('Importancia')
            plt.title('Importancia - Modelo Hurdle')
            plt.gca().invert_yaxis()
            
            # Subplot para modelo de regresi√≥n
            plt.subplot(1, 2, 2)
            top_reg = reg_importance_df.head(15)
            plt.barh(range(len(top_reg)), top_reg['importance'])
            plt.yticks(range(len(top_reg)), top_reg['feature'])
            plt.xlabel('Importancia')
            plt.title('Importancia - Modelo Regresi√≥n')
            plt.gca().invert_yaxis()
            
            plt.tight_layout()
            plt.show()

def analyze_population_stability(X_train, X_test, feature_cols=None, n_bins=10):
    """
    Calcula el Population Stability Index (PSI) para detectar drift.
    """
    if feature_cols is None:
        feature_cols = X_train.select_dtypes(include=[np.number]).columns
    
    psi_results = {}
    
    for col in feature_cols:
        if col in X_train.columns and col in X_test.columns:
            # Crear bins basados en los datos de entrenamiento
            _, bin_edges = np.histogram(X_train[col], bins=n_bins)
            
            # Calcular distribuciones
            train_dist, _ = np.histogram(X_train[col], bins=bin_edges, density=True)
            test_dist, _ = np.histogram(X_test[col], bins=bin_edges, density=True)
            
            # Normalizar para obtener proporciones
            train_prop = train_dist / train_dist.sum()
            test_prop = test_dist / test_dist.sum()
            
            # Evitar log(0) a√±adiendo peque√±o epsilon
            epsilon = 1e-10
            train_prop = np.maximum(train_prop, epsilon)
            test_prop = np.maximum(test_prop, epsilon)
            
            # Calcular PSI
            psi = np.sum((test_prop - train_prop) * np.log(test_prop / train_prop))
            psi_results[col] = psi
    
    return psi_results

if df_full is not None and 'X_train' in locals():
    print("\n" + "="*70)
    print("AN√ÅLISIS DE ESTABILIDAD POBLACIONAL")
    print("="*70)
    
    # Seleccionar caracter√≠sticas originales para an√°lisis de estabilidad
    original_features = ['limit_bal', 'age', 'bill_amt_sept', 'pay_amt_sept', 
                        'pay_sept', 'sex', 'education', 'marriage']
    available_features = [col for col in original_features if col in X_train.columns and col in X_test.columns]
    
    if available_features:
        psi_scores = analyze_population_stability(X_train, X_test, available_features)
        
        # Crear DataFrame de resultados PSI
        psi_df = pd.DataFrame([
            {'feature': k, 'psi': v, 'stability': 
             'Estable' if v < 0.1 else 'Moderado' if v < 0.2 else 'Inestable'} 
            for k, v in psi_scores.items()
        ]).sort_values('psi', ascending=False)
        
        print("Population Stability Index (PSI) por caracter√≠stica:")
        print("PSI < 0.1: Estable | 0.1-0.2: Moderado | >0.2: Inestable")
        print("-" * 60)
        for _, row in psi_df.iterrows():
            print(f"{row['feature']:20} | {row['psi']:6.3f} | {row['stability']}")
        
        # Visualizaci√≥n de estabilidad
        plt.figure(figsize=(12, 6))
        colors = ['green' if x < 0.1 else 'orange' if x < 0.2 else 'red' 
                 for x in psi_df['psi']]
        
        plt.barh(range(len(psi_df)), psi_df['psi'], color=colors)
        plt.yticks(range(len(psi_df)), psi_df['feature'])
        plt.xlabel('Population Stability Index (PSI)')
        plt.title('An√°lisis de Estabilidad de Variables')
        plt.axvline(x=0.1, color='orange', linestyle='--', alpha=0.7, label='Umbral Moderado')
        plt.axvline(x=0.2, color='red', linestyle='--', alpha=0.7, label='Umbral Inestable')
        plt.legend()
        plt.grid(True, alpha=0.3)
        plt.tight_layout()
        plt.show()
        
        # Recomendaciones basadas en PSI
        unstable_features = psi_df[psi_df['psi'] > 0.2]['feature'].tolist()
        if unstable_features:
            print(f"\n‚ö†Ô∏è  ALERTA: Las siguientes variables muestran drift significativo:")
            for feature in unstable_features:
                print(f"   - {feature}")
            print("\nRecomendaciones:")
            print("1. Reentrenar el modelo con datos m√°s recientes")
            print("2. Considerar recalibraci√≥n de probabilidades")
            print("3. Implementar monitoreo continuo de estas variables")

print("\n" + "="*70)
print("RESUMEN EJECUTIVO")
print("="*70)

if 'best_model_name' in locals():
    print(f"\nüéØ MODELO DE CLASIFICACI√ìN SELECCIONADO: {best_model_name.upper()}")
    print(f"   ‚Ä¢ ROC-AUC: {model_scores[best_model_name]['roc_auc']:.3f}")
    print(f"   ‚Ä¢ Precision@10%: {model_scores[best_model_name].get('precision_at_0.1', 0):.3f}")
    print(f"   ‚Ä¢ Profit M√°ximo: ${model_scores[best_model_name]['max_profit']:,.0f}")
    print(f"   ‚Ä¢ Threshold √ìptimo: {best_threshold:.3f}")

if 'r2_hurdle' in locals():
    print(f"\nüìà MODELO DE REGRESI√ìN HURDLE:")
    print(f"   ‚Ä¢ R¬≤ Score: {r2_hurdle:.3f}")
    print(f"   ‚Ä¢ RMSE: ${rmse_hurdle:,.0f}")
    print(f"   ‚Ä¢ MAPE: {mape_hurdle:.1f}%")
    print(f"   ‚Ä¢ ROC-AUC Hurdle: {hurdle_auc:.3f}")

print(f"\nüìä MEJORAS IMPLEMENTADAS:")
print("   ‚úÖ Validaci√≥n temporal para evitar data leakage")
print("   ‚úÖ Ingenier√≠a de caracter√≠sticas sin informaci√≥n futura")
print("   ‚úÖ Manejo adaptativo de outliers con Isolation Forest")
print("   ‚úÖ Ensemble diversificado con m√∫ltiples algoritmos")
print("   ‚úÖ M√©tricas orientadas al negocio (profit optimization)")
print("   ‚úÖ An√°lisis de estabilidad poblacional (PSI)")
print("   ‚úÖ Calibraci√≥n de probabilidades mejorada")

print(f"\nüöÄ RECOMENDACIONES PARA PRODUCCI√ìN:")
print("   1. Implementar monitoreo continuo de PSI")
print("   2. Recalibrar modelos mensualmente")
print("   3. Validar performance en cohortes temporales")
print("   4. Establecer alertas autom√°ticas por drift")
print("   5. Documentar decisiones para auditor√≠a regulatoria")
print("   6. Implementar A/B testing para nuevas versiones")

print(f"\nüí° PR√ìXIMOS PASOS:")
print("   ‚Ä¢ Integraci√≥n con pipeline de MLOps")
print("   ‚Ä¢ Desarrollo de API REST para scoring")
print("   ‚Ä¢ Dashboard de monitoreo en tiempo real")
print("   ‚Ä¢ Documentaci√≥n para compliance regulatorio")

print("\n" + "="*70)
print("AN√ÅLISIS COMPLETADO ‚úÖ")
print("="*70)

# --- C√≥digo extra√≠do de: 03_Modelado_Clasificacion.ipynb ---
import pandas as pd
import numpy as np
from pathlib import Path
import matplotlib.pyplot as plt
import seaborn as sns

# Se a√±ade TimeSeriesSplit a las importaciones
from sklearn.model_selection import train_test_split, TimeSeriesSplit, RandomizedSearchCV
from sklearn.preprocessing import StandardScaler
from imblearn.over_sampling import SMOTE
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from sklearn.metrics import classification_report, roc_auc_score, accuracy_score, f1_score, precision_recall_curve, auc
from sklearn.base import clone # Importaci√≥n clave para la soluci√≥n

try:
    project_root = Path.cwd().parent
    processed_path = project_root / "data" / "processed"
    
    df_full = pd.read_csv(processed_path / "features_clasificacion.csv")
    df_reduced = pd.read_csv(processed_path / "features_reducido_clasificacion.csv")
    
    print("Datasets cargados exitosamente.")
    print(f"Forma del dataset completo: {df_full.shape}")
    print(f"Forma del dataset reducido: {df_reduced.shape}")

except FileNotFoundError as e:
    print(f"Error: No se encontr√≥ el archivo. Verifica la ruta: {e}")
    df_full = None
    df_reduced = None

def train_and_evaluate_model(X_train, X_test, y_train, y_test, model, model_name, dataset_name):
    """
    Entrena y eval√∫a un modelo de clasificaci√≥n en datos ya divididos temporalmente.
    """
    feature_names_list = X_train.columns.tolist()

    smote = SMOTE(random_state=42)
    X_train_res, y_train_res = smote.fit_resample(X_train, y_train)

    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train_res)
    X_test_scaled = scaler.transform(X_test)

    model.fit(X_train_scaled, y_train_res)

    y_pred = model.predict(X_test_scaled)
    y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]

    metrics = {
        'Model': model_name,
        'Dataset': dataset_name,
        'Accuracy': accuracy_score(y_test, y_pred),
        'ROC-AUC': roc_auc_score(y_test, y_pred_proba),
        'F1-Score (Default)': f1_score(y_test, y_pred, pos_label=1),
        'artifacts': {
            'feature_names': feature_names_list,
            'y_test': y_test,
            'y_pred_proba': y_pred_proba,
            'model': model,
            
            # --- L√çNEAS A√ëADIDAS DE VUELTA ---
            # Guardar los datos necesarios para la optimizaci√≥n de hiperpar√°metros.
            'X_train_scaled': X_train_scaled,
            'y_train_res': y_train_res,
            'X_test_scaled': X_test_scaled
            # --- FIN DE LAS L√çNEAS A√ëADIDAS ---
        }
    }

    print(f"\n--- Resultados para {model_name} en {dataset_name} ---")
    print(classification_report(y_test, y_pred, target_names=['No Incumplimiento', 'Incumplimiento']))

    return metrics

if df_full is not None and df_reduced is not None:
    
    datasets_dfs = {
        "Completo": df_full,
        "Reducido": df_reduced
    }
    
    # Se renombra a "prototypes" para indicar que son plantillas a clonar
    model_prototypes = {
        "Regresi√≥n Log√≠stica": LogisticRegression(random_state=42, max_iter=1000),
        "Random Forest": RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1),
        "XGBoost": XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)
    }
    
    results = []
    
    for d_name, df_data in datasets_dfs.items():
        df_sorted = df_data.sort_values('ID').reset_index(drop=True)
        
        split_point = int(len(df_sorted) * 0.8)
        train_df = df_sorted.iloc[:split_point]
        test_df = df_sorted.iloc[split_point:]
        
        target_col = 'default_payment_next_month'
        features_to_use = df_data.select_dtypes(include=np.number).columns.drop([target_col, 'ID'], errors='ignore').tolist()
        
        X_train = train_df[features_to_use]
        y_train = train_df[target_col]
        X_test = test_df[features_to_use]
        y_test = test_df[target_col]
        
        # Iterar sobre cada prototipo de modelo
        for m_name, model_proto in model_prototypes.items():
            # --- SOLUCI√ìN IMPLEMENTADA ---
            # Clonar el modelo para asegurar una instancia limpia en cada entrenamiento
            model_instance = clone(model_proto)
            
            metrics = train_and_evaluate_model(X_train, X_test, y_train, y_test, model_instance, m_name, d_name)
            results.append(metrics)
            
    results_df = pd.DataFrame(results)
    
    print("\n--- Tabla Comparativa de Rendimiento de Modelos ---")
    display(results_df.drop(columns=['artifacts']))

if 'results_df' in locals() and not results_df.empty:
    best_result = results_df.loc[results_df['F1-Score (Default)'].idxmax()]
    best_model_name = best_result['Model']
    best_dataset_name = best_result['Dataset']
    
    print(f"\n--- An√°lisis Profundo del Mejor Modelo: {best_model_name} en Dataset {best_dataset_name} ---")
    
    artifacts = best_result['artifacts']
    model_best = artifacts['model']
    feature_names = artifacts['feature_names']
    y_test_best = artifacts['y_test']
    y_pred_proba_best = artifacts['y_pred_proba']

    if hasattr(model_best, 'feature_importances_'):
        importances = model_best.feature_importances_
        
        # Ahora las longitudes coincidir√°n perfectamente
        feature_importance_df = pd.DataFrame({'feature': feature_names, 'importance': importances})
        feature_importance_df = feature_importance_df.sort_values(by='importance', ascending=False)
        
        plt.figure(figsize=(10, 12))
        sns.barplot(x='importance', y='feature', data=feature_importance_df.head(20), palette='mako')
        plt.title(f'Top 20 Caracter√≠sticas m√°s Importantes ({best_model_name} - {best_dataset_name})')
        plt.xlabel('Importancia')
        plt.ylabel('Caracter√≠stica')
        plt.tight_layout()
        plt.show()
        
    precision, recall, _ = precision_recall_curve(y_test_best, y_pred_proba_best)
    pr_auc = auc(recall, precision)
    
    plt.figure(figsize=(8, 6))
    plt.plot(recall, precision, color='darkorange', lw=2, label=f'Curva PR (√°rea = {pr_auc:.2f})')
    plt.xlabel('Recall')
    plt.ylabel('Precisi√≥n')
    plt.title('Curva de Precisi√≥n-Recall del Mejor Modelo')
    plt.legend(loc="lower left")
    plt.grid(True)
    plt.show()

if 'results_df' in locals() and not results_df.empty:
    print("\n--- Iniciando Optimizaci√≥n de Hiperpar√°metros para Random Forest ---")
    
    # Usar los artefactos del mejor modelo identificado
    artifacts_best = results_df.loc[results_df['F1-Score (Default)'].idxmax()]['artifacts']
    X_train_scaled_best = artifacts_best['X_train_scaled']
    y_train_res_best = artifacts_best['y_train_res']
    X_test_scaled_best = artifacts_best['X_test_scaled']
    y_test_best = artifacts_best['y_test']
    
    param_distributions = {
        'n_estimators': [100, 200, 300, 400],
        'max_depth': [10, 20, 30, None],
        'min_samples_split': [2, 5, 10],
        'min_samples_leaf': [1, 2, 4],
        'max_features': ['sqrt', 'log2']
    }
    
    rf_clf = RandomForestClassifier(random_state=42, n_jobs=-1)
    tscv = TimeSeriesSplit(n_splits=3)
    
    random_search = RandomizedSearchCV(
        estimator=rf_clf,
        param_distributions=param_distributions,
        n_iter=50,
        cv=tscv,
        scoring='f1',
        verbose=2,
        random_state=42,
        n_jobs=-1
    )
    
    random_search.fit(X_train_scaled_best, y_train_res_best)
    
    print("\n--- Mejores Hiperpar√°metros Encontrados ---")
    print(random_search.best_params_)
    
    best_rf_clf = random_search.best_estimator_
    y_pred_best_rf = best_rf_clf.predict(X_test_scaled_best)
    y_pred_proba_best_rf = best_rf_clf.predict_proba(X_test_scaled_best)[:, 1]
    
    print("\n--- M√©tricas de Evaluaci√≥n del Modelo Random Forest OPTIMIZADO ---")
    print(f"Accuracy: {accuracy_score(y_test_best, y_pred_best_rf):.4f}")
    print(f"ROC-AUC Score: {roc_auc_score(y_test_best, y_pred_proba_best_rf):.4f}")
    print("\nReporte de Clasificaci√≥n (Optimizado):")
    print(classification_report(y_test_best, y_pred_best_rf, target_names=['No Incumplimiento', 'Incumplimiento']))

# --- C√≥digo extra√≠do de: 01_Analisis_Exploratorio.ipynb ---
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path

sns.set_style("whitegrid")
plt.rcParams['figure.figsize'] = (10, 6)

try:
    project_root = Path.cwd().parent
    data_file_path = project_root / "data" / "raw" / "default_of_credit_card_clients.xls"
    
    if data_file_path.exists():
        print(f"Archivo encontrado en: {data_file_path}")
        # Se lee el archivo Excel, omitiendo la primera fila que no contiene datos.
        df_original = pd.read_excel(data_file_path, skiprows=1)
    else:
        print(f"Error: No se encontr√≥ el archivo en la ruta esperada: {data_file_path}")
        df_original = None

except Exception as e:
    print(f"Ocurri√≥ un error al cargar el archivo: {e}")
    df_original = None

if df_original is not None:
    print("\n--- Vista Previa de los Datos Originales ---")
    display(df_original.head())
    
    print("\n--- Informaci√≥n General del DataFrame ---")
    df_original.info()
else:
    print("\nNo se pudo cargar el DataFrame. El an√°lisis no puede continuar.")

df = df_original.copy()

df.rename(columns={
    'LIMIT_BAL': 'limit_bal', 'SEX': 'sex', 'EDUCATION': 'education', 'MARRIAGE': 'marriage', 'AGE': 'age',
    'PAY_0': 'pay_sept', 'PAY_2': 'pay_aug', 'PAY_3': 'pay_july', 'PAY_4': 'pay_june', 'PAY_5': 'pay_may', 'PAY_6': 'pay_april',
    'BILL_AMT1': 'bill_amt_sept', 'BILL_AMT2': 'bill_amt_aug', 'BILL_AMT3': 'bill_amt_july', 'BILL_AMT4': 'bill_amt_june', 'BILL_AMT5': 'bill_amt_may', 'BILL_AMT6': 'bill_amt_april',
    'PAY_AMT1': 'pay_amt_sept', 'PAY_AMT2': 'pay_amt_aug', 'PAY_AMT3': 'pay_amt_july', 'PAY_AMT4': 'pay_amt_june', 'PAY_AMT5': 'pay_amt_may', 'PAY_AMT6': 'pay_amt_april',
    'default payment next month': 'default_payment_next_month'
}, inplace=True)

# 'education': Se agrupan valores no documentados (0, 5, 6) en la categor√≠a 'Otros' (4).
df['education'] = df['education'].replace([0, 5, 6], 4)
# 'marriage': Se agrupa el valor no documentado (0) en 'Otros' (3).
df['marriage'] = df['marriage'].replace(0, 3)

print("Education:", sorted(df['education'].unique()))
print("Marriage:", sorted(df['marriage'].unique()))

display(df.describe())

default_counts = df['default_payment_next_month'].value_counts()
print(default_counts)

plt.figure(figsize=(6, 4))
sns.countplot(x='default_payment_next_month', data=df, palette='viridis')
plt.title('Distribuci√≥n de Clientes por Incumplimiento de Pago')
plt.xlabel('Incumplimiento (0 = No, 1 = S√≠)')
plt.ylabel('Cantidad de Clientes')
plt.xticks([0, 1], ['No Incumplimiento (78%)', 'Incumplimiento (22%)'])
plt.show()

df['sex_label'] = df['sex'].map({1: 'Hombre', 2: 'Mujer'})
df['education_label'] = df['education'].map({1: 'Posgrado', 2: 'Universidad', 3: 'Secundaria', 4: 'Otros'})
df['marriage_label'] = df['marriage'].map({1: 'Casado/a', 2: 'Soltero/a', 3: 'Otros'})

fig, axes = plt.subplots(1, 3, figsize=(18, 5))
sns.countplot(x='sex_label', data=df, ax=axes[0], palette='pastel')
axes[0].set_title('Distribuci√≥n por G√©nero')
sns.countplot(x='education_label', data=df, ax=axes[1], palette='pastel', order=df['education_label'].value_counts().index)
axes[1].set_title('Distribuci√≥n por Nivel Educativo')
axes[1].tick_params(axis='x', rotation=45)
sns.countplot(x='marriage_label', data=df, ax=axes[2], palette='pastel')
axes[2].set_title('Distribuci√≥n por Estado Civil')
plt.suptitle('An√°lisis de Variables Demogr√°ficas', fontsize=16)
plt.tight_layout(rect=[0, 0.03, 1, 0.95])
plt.show()

fig, axes = plt.subplots(1, 2, figsize=(15, 5))
sns.histplot(df['limit_bal'], kde=True, ax=axes[0], bins=30, color='skyblue')
axes[0].set_title('Distribuci√≥n del L√≠mite de Cr√©dito')
axes[0].set_xlabel('L√≠mite de Cr√©dito (NT$)')
sns.histplot(df['age'], kde=True, ax=axes[1], bins=30, color='salmon')
axes[1].set_title('Distribuci√≥n de la Edad')
axes[1].set_xlabel('Edad')
plt.suptitle('An√°lisis de Variables Num√©ricas', fontsize=16)
plt.show()

df['pay_sept_label'] = df['pay_sept'].map({
    -1: 'Pago puntual', 0: 'Pago m√≠nimo', 1: 'Retraso 1 mes',
    2: 'Retraso 2 meses', 3: 'Retraso 3 meses', 4: 'Retraso 4 meses',
    5: 'Retraso 5 meses', 6: 'Retraso 6 meses', 7: 'Retraso 7 meses',
    8: 'Retraso >8 meses'
}).fillna('Otro')

plt.figure(figsize=(12, 7))
sns.countplot(y='pay_sept_label', data=df, order=df['pay_sept_label'].value_counts().index, palette='plasma')
plt.title('Estado de Pago en Septiembre de 2005')
plt.xlabel('Cantidad de Clientes')
plt.ylabel('Estado del Pago')
plt.show()

corr_cols = ['limit_bal', 'age'] + [c for c in df.columns if 'bill_amt' in c] + [c for c in df.columns if 'pay_amt' in c] + ['default_payment_next_month']
correlation_matrix = df[corr_cols].corr()

plt.figure(figsize=(12, 10))
sns.heatmap(correlation_matrix, cmap='viridis', annot=False)
plt.title('Matriz de Correlaci√≥n', fontsize=16)
plt.show()

num_cols = ['limit_bal'] + [c for c in df.columns if c.startswith('bill_amt_')] + [c for c in df.columns if c.startswith('pay_amt_')]

print("\n--- Cuantiles Extremos de Variables Financieras ---")
v_q = df[num_cols].quantile([0, 0.01, 0.25, 0.5, 0.75, 0.99, 1]).T
display(v_q)

# Boxplots agrupados para una visualizaci√≥n compacta.
fig, axes = plt.subplots(3, 5, figsize=(20, 12))
axes = axes.flatten()

for i, col in enumerate(num_cols):
    sns.boxplot(x=df[col], ax=axes[i], color='lightblue')
    axes[i].set_title(col, fontsize=10)
    axes[i].set_xlabel('')

# Ocultar ejes sobrantes si el n√∫mero de columnas no es m√∫ltiplo de 5.
for j in range(i + 1, len(axes)):
    axes[j].axis('off')

plt.suptitle('Detecci√≥n de Valores At√≠picos en Variables Financieras', fontsize=18)
plt.tight_layout(rect=[0, 0.03, 1, 0.97])
plt.show()

# An√°lisis temporal agregado por mes.
bill_cols = [c for c in df.columns if c.startswith('bill_amt_')]
pay_cols  = [c for c in df.columns if c.startswith('pay_amt_')]
month_names = ['Sept', 'Aug', 'July', 'June', 'May', 'April']

df_ts = pd.DataFrame({
    'facturacion': df[bill_cols].sum().values,
    'pagos'      : df[pay_cols].sum().values
}, index=month_names)

df_ts.plot(marker='o', figsize=(10, 5))
plt.title('Facturaci√≥n Total vs. Pagos Totales por Mes')
plt.xlabel('Mes (2005)')
plt.ylabel('Monto Total (NT$)')
plt.grid(True, which='both', linestyle='--')
plt.show()

try:
    processed_data_path = project_root / "data" / "processed"
    processed_data_path.mkdir(parents=True, exist_ok=True)
    
    # Se eliminan las columnas de etiquetas antes de guardar.
    df_to_save = df.drop(columns=['sex_label', 'education_label', 'marriage_label'])
    
    file_path = processed_data_path / "credit_card_clients_clean.csv"
    df_to_save.to_csv(file_path, index=False)
    
    print(f"\nDataFrame limpio guardado exitosamente en: {file_path}")

except Exception as e:
    print(f"\nOcurri√≥ un error al guardar el archivo: {e}")

try:
    project_root = Path.cwd().parent
    processed_data_path = project_root / "data" / "processed"
    
    processed_data_path.mkdir(parents=True, exist_ok=True)
    
    file_path = processed_data_path / "credit_card_clients_clean.csv"
    df_clean.to_csv(file_path, index=False)
    
    print(f"\ DataFrame limpio guardado exitosamente en: {file_path}")

except Exception as e:
    print(f"\ Ocurri√≥ un error al guardar el archivo: {e}")

# --- C√≥digo extra√≠do de: 04_Modelado_Regresion.ipynb ---
import pandas as pd
import numpy as np
from pathlib import Path
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression, HuberRegressor
from sklearn.ensemble import RandomForestRegressor
from xgboost import XGBRegressor
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
from sklearn.base import clone

sns.set_style("whitegrid")
plt.rcParams['figure.figsize'] = (10, 6)

try:
    project_root = Path.cwd().parent
    features_path = project_root / "data" / "processed" / "features_regresion.csv"
    df_reg = pd.read_csv(features_path)
    print(f"DataFrame para regresi√≥n cargado desde: {features_path}")
except FileNotFoundError:
    print(f"Error: No se encontr√≥ el archivo en: {features_path}")
    df_reg = None

def train_and_evaluate_regressor(X_train, y_train, X_test, y_test, model, model_name, use_log_transform=False):
    """
    Entrena un modelo de regresi√≥n, realiza predicciones y devuelve las m√©tricas y artefactos.
    """
    # Guardar los nombres de las caracter√≠sticas para el an√°lisis posterior
    feature_names = X_train.columns.tolist()
    
    # Aplicar transformaci√≥n logar√≠tmica si se especifica
    y_train_target = y_train
    if use_log_transform:
        y_train_target = np.log1p(y_train)

    # Entrenamiento
    model.fit(X_train, y_train_target)
    
    # Predicci√≥n
    y_pred = model.predict(X_test)
    
    # Transformaci√≥n inversa si es necesario
    if use_log_transform:
        y_pred = np.expm1(y_pred)
    
    # C√°lculo de m√©tricas
    r2 = r2_score(y_test, y_pred)
    rmse = np.sqrt(mean_squared_error(y_test, y_pred))
    mae = mean_absolute_error(y_test, y_pred)
    
    transform_label = "Con Log" if use_log_transform else "Sin Log"
    print(f"\n--- Resultados para {model_name} ({transform_label}) ---")
    print(f"R¬≤ Score: {r2:.4f}")
    print(f"RMSE: {rmse:.4f}")
    print(f"MAE: {mae:.4f}")
    
    metrics = {
        'Model': model_name,
        'Transformation': transform_label,
        'R2 Score': r2,
        'RMSE': rmse,
        'MAE': mae,
        'artifacts': {
            'model': model,
            'feature_names': feature_names
        }
    }
    
    return metrics, y_pred

if df_reg is not None:
    # 1. Divisi√≥n Temporal de Datos
    df_reg_sorted = df_reg.sort_values('ID').reset_index(drop=True)
    split_point = int(len(df_reg_sorted) * 0.8)
    train_df = df_reg_sorted.iloc[:split_point]
    test_df = df_reg_sorted.iloc[split_point:]

    # 2. Definir X e y, excluyendo el ID de las caracter√≠sticas
    target_col = 'pay_amt_june'
    features_to_use = df_reg.select_dtypes(include=np.number).columns.drop([target_col, 'ID'], errors='ignore').tolist()
    
    X_train = train_df[features_to_use]
    y_train = train_df[target_col]
    X_test = test_df[features_to_use]
    y_test = test_df[target_col]

    # 3. Escalado de Caracter√≠sticas (se pasa un DataFrame con nombres de columnas)
    scaler = StandardScaler()
    X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train), columns=features_to_use)
    X_test_scaled = pd.DataFrame(scaler.transform(X_test), columns=features_to_use)

    # 4. Definir Prototipos de Modelos
    model_prototypes = {
        "Regresi√≥n Lineal": LinearRegression(),
        "Regresi√≥n de Huber": HuberRegressor(max_iter=1000),
        "Random Forest": RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1),
        "XGBoost": XGBRegressor(objective='reg:squarederror', random_state=42, n_jobs=-1)
    }
    
    results = []
    predictions = {}

    # 5. Iterar, Clonar y Entrenar
    for name, model_proto in model_prototypes.items():
        # Experimento SIN transformaci√≥n logar√≠tmica
        model_no_log = clone(model_proto)
        metrics_no_log, y_pred_no_log = train_and_evaluate_regressor(X_train_scaled, y_train, X_test_scaled, y_test, model_no_log, name, use_log_transform=False)
        results.append(metrics_no_log)
        predictions[f"{name} (Sin Log)"] = y_pred_no_log
        
        # Experimento CON transformaci√≥n logar√≠tmica
        model_log = clone(model_proto)
        metrics_log, y_pred_log = train_and_evaluate_regressor(X_train_scaled, y_train, X_test_scaled, y_test, model_log, name, use_log_transform=True)
        results.append(metrics_log)
        predictions[f"{name} (Con Log)"] = y_pred_log

    # 6. Tabla Comparativa de Resultados
    results_df = pd.DataFrame(results).sort_values(by='R2 Score', ascending=False)
    
    print("\n--- Tabla Comparativa de Rendimiento de Modelos de Regresi√≥n ---")
    display(results_df.drop(columns=['artifacts']))

if 'results_df' in locals() and not results_df.empty:
    best_row = results_df.loc[results_df['R2 Score'].idxmax()]
    best_model_name = best_row['Model']
    best_transform_label = best_row['Transformation']
    best_prediction_key = f"{best_model_name} ({best_transform_label})"
    
    print(f"\n--- An√°lisis Profundo del Mejor Modelo: {best_model_name} ({best_transform_label}) ---")
    
    # 1. Gr√°fico de Predicciones vs. Valores Reales
    plt.figure(figsize=(8, 8))
    sns.scatterplot(x=y_test, y=predictions[best_prediction_key], alpha=0.5)
    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], '--r', linewidth=2)
    plt.title(f'Valores Reales vs. Predicciones ({best_model_name} - {best_transform_label})')
    plt.xlabel('Valores Reales (pay_amt_june)')
    plt.ylabel('Predicciones')
    plt.show()
    
    # 2. Importancia de Caracter√≠sticas (usando artefactos)
    artifacts = best_row['artifacts']
    best_model_instance = artifacts['model']
    feature_names = artifacts['feature_names']

    if hasattr(best_model_instance, 'feature_importances_'):
        importances = best_model_instance.feature_importances_
        
        feature_importance_df = pd.DataFrame({'feature': feature_names, 'importance': importances})
        feature_importance_df = feature_importance_df.sort_values(by='importance', ascending=False)
        
        plt.figure(figsize=(10, 10))
        sns.barplot(x='importance', y='feature', data=feature_importance_df.head(20), palette='viridis')
        plt.title(f'Top 20 Caracter√≠sticas m√°s Importantes ({best_model_name})')
        plt.xlabel('Importancia')
        plt.ylabel('Caracter√≠stica')
        plt.tight_layout()
        plt.show()

# --- C√≥digo extra√≠do de: 02_Ingenieria_de_Caracteristicas.ipynb ---
import pandas as pd
import numpy as np
from pathlib import Path
from statsmodels.stats.outliers_influence import variance_inflation_factor
from statsmodels.tools.tools import add_constant

try:
    project_root = Path.cwd().parent
    clean_data_path = project_root / "data" / "processed" / "credit_card_clients_clean.csv"
    df = pd.read_csv(clean_data_path)
    print(f"DataFrame limpio cargado desde: {clean_data_path}")
except FileNotFoundError:
    print(f"Error: No se encontr√≥ el archivo: {clean_data_path}")
    df = None

# --- 1. Correcci√≥n de Fuga de Datos y Nueva Ingenier√≠a de Caracter√≠sticas ---

# Copiamos el dataframe limpio para empezar
df_features = df.copy()

# Definir grupos de columnas HIST√ìRICAS para evitar data leakage
# Usamos datos de Agosto hacia atr√°s para predecir el default del mes siguiente (Octubre)
bill_amt_cols_hist = ['bill_amt_aug', 'bill_amt_july', 'bill_amt_june', 'bill_amt_may', 'bill_amt_april']
pay_amt_cols_hist = ['pay_amt_aug', 'pay_amt_july', 'pay_amt_june', 'pay_amt_may', 'pay_amt_april']
pay_status_cols_hist = ['pay_aug', 'pay_july', 'pay_june', 'pay_may', 'pay_april']

# -- A. Ratios de Salud Financiera (Corregido) --
epsilon = 1e-6
# Tasa de utilizaci√≥n del mes m√°s reciente (Septiembre)
df_features['utilization_sept'] = df_features['bill_amt_sept'] / (df_features['limit_bal'] + epsilon)

# -- B. Agregados de Historial Financiero (Corregido) --
df_features['bill_amt_avg_hist'] = df_features[bill_amt_cols_hist].mean(axis=1)
df_features['bill_amt_std_hist'] = df_features[bill_amt_cols_hist].std(axis=1)
df_features['pay_amt_avg_hist'] = df_features[pay_amt_cols_hist].mean(axis=1)

# -- C. Caracter√≠sticas de Tendencia (Corregido) --
# Se calcula la pendiente sobre datos hist√≥ricos para observar tendencias
def calculate_slope(row, cols):
    y = row[cols].values.astype(float)
    x = np.array(range(len(y)))
    slope, _ = np.polyfit(x, y, 1)
    return slope

# Aplicamos la funci√≥n sobre las columnas hist√≥ricas en orden cronol√≥gico (de m√°s antiguo a m√°s reciente)
df_features['bill_amt_slope'] = df_features.apply(lambda row: calculate_slope(row, bill_amt_cols_hist[::-1]), axis=1)
df_features['pay_status_slope'] = df_features.apply(lambda row: calculate_slope(row, pay_status_cols_hist[::-1]), axis=1)

# -- D. Indicadores de Comportamiento de Pago (Corregido) --
df_features['pay_status_avg_hist'] = df_features[pay_status_cols_hist].mean(axis=1)
df_features['months_with_delay_hist'] = (df_features[pay_status_cols_hist] > 0).sum(axis=1)

# -- E. Nuevas Caracter√≠sticas Propuestas --

# Volatilidad de pagos: Un cliente con pagos muy irregulares puede ser riesgoso
all_pay_amt_cols = ['pay_amt_sept'] + pay_amt_cols_hist
df_features['payment_volatility'] = df_features[all_pay_amt_cols].std(axis=1) / (df_features[all_pay_amt_cols].mean(axis=1) + epsilon)

# Proxy de relaci√≥n deuda-ingreso: Compara la deuda promedio con el l√≠mite total
df_features['debt_to_limit_ratio'] = df_features['bill_amt_avg_hist'] / (df_features['limit_bal'] + epsilon)

# Reemplazar valores infinitos o NaN que puedan surgir de las divisiones
df_features.replace([np.inf, -np.inf], np.nan, inplace=True)
df_features.fillna(0, inplace=True) # O una estrategia de imputaci√≥n m√°s sofisticada

print("--- Ingenier√≠a de Caracter√≠sticas Mejorada (sin fuga de datos) ---")
display(df_features.head())

if df is not None:
    # Se seleccionan solo las columnas num√©ricas para el c√°lculo de VIF.
    numeric_cols_for_vif = df_features.select_dtypes(include=np.number).drop(columns=['ID', 'default_payment_next_month'])
    
    # Se a√±ade una constante para el c√°lculo correcto del VIF.
    X_vif = add_constant(numeric_cols_for_vif)
    
    vif_data = pd.DataFrame()
    vif_data["feature"] = X_vif.columns
    vif_data["VIF"] = [variance_inflation_factor(X_vif.values, i) for i in range(X_vif.shape[1])]
    
    print("\n--- An√°lisis de Factor de Inflaci√≥n de la Varianza (VIF) ---")
    # Se muestran las 15 variables con mayor VIF, que son las m√°s problem√°ticas.
    display(vif_data.sort_values(by='VIF', ascending=False).head(15))

if df is not None:
    features_reducidas = [
        'ID',
        # Demogr√°ficas y L√≠mite de Cr√©dito
        'limit_bal', 'sex', 'education', 'marriage', 'age',
        # Comportamiento de Pago (las m√°s predictivas)
        'pay_sept', 'pay_status_slope',
        # Salud Financiera (menos correlacionadas)
        'utilization_sept', 'bill_amt_std_hist', 'pay_amt_avg_hist',
        # Variable Objetivo
        'default_payment_next_month'
    ]
    df_reducido = df_features[features_reducidas]
    print("\n--- Dataset Reducido Creado ---")
    display(df_reducido.head())

if df is not None:
    df_features_reg = df.copy()
    
    # Se usan solo datos hasta junio para evitar fuga de informaci√≥n.
    bill_cols_reg = ['bill_amt_june', 'bill_amt_may', 'bill_amt_april']
    pay_cols_reg = ['pay_amt_may', 'pay_amt_april']
    
    # 6.1. Ratios y Agregados
    df_features_reg['utilization_june'] = df_features_reg['bill_amt_june'] / (df_features_reg['limit_bal'] + epsilon)
    df_features_reg['bill_amt_avg_3m'] = df_features_reg[bill_cols_reg].mean(axis=1)
    df_features_reg['pay_amt_avg_2m'] = df_features_reg[pay_cols_reg].mean(axis=1)

    print("\n--- Ingenier√≠a de Caracter√≠sticas para Regresi√≥n completada ---")

if df is not None:
    try:
        processed_data_path = project_root / "data" / "processed"
        
        # Guardar dataset completo para clasificaci√≥n
        path_clasificacion = processed_data_path / "features_clasificacion.csv"
        df_features.to_csv(path_clasificacion, index=False)
        print(f"\nDataset para clasificaci√≥n guardado en: {path_clasificacion}")

        # Guardar dataset reducido para clasificaci√≥n
        path_reducido = processed_data_path / "features_reducido_clasificacion.csv"
        df_reducido.to_csv(path_reducido, index=False)
        print(f"Dataset reducido para clasificaci√≥n guardado en: {path_reducido}")
        
        # Guardar dataset para regresi√≥n
        path_regresion = processed_data_path / "features_regresion.csv"
        df_features_reg.to_csv(path_regresion, index=False)
        print(f"Dataset para regresi√≥n guardado en: {path_regresion}")

    except Exception as e:
        print(f"\nOcurri√≥ un error al guardar los archivos: {e}")

